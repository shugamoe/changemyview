{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7 - Information Extraction\n",
    "\n",
    "\n",
    "This week, we move from arbitrary textual classification to the use of computation and linguistic models to parse precise claims from documents. Rather than focusing on simply the *ideas* in a corpus, here we focus on understanding and extracting its precise *claims*. This process involves a sequential pipeline of classifying and structuring tokens from text, each of which generates potentially useful data for the content analyst. Steps in this process, which we examine in this notebook, include: 1) tagging words by their part of speech (POS) to reveal the linguistic role they play in the sentence (e.g., Verb, Noun, Adjective, etc.); 2) tagging words as named entities (NER) such as places or organizations; 3) structuring or \"parsing\" sentences into nested phrases that are local to, describe or depend on one another; and 4) extracting informational claims from those phrases, like the Subject-Verb-Object (SVO) triples we extract here. While much of this can be done directly in the python package NLTK that we introduced in week 2, here we use NLTK bindings to the Stanford NLP group's open software, written in Java. Try typing a sentence into the online version [here]('http://nlp.stanford.edu:8080/corenlp/') to get a sense of its potential. It is superior in performance to NLTK's implementations, but takes time to run, and so for these exercises we will parse and extract information for a very small text corpus. Of course, for final projects that draw on these tools, we encourage you to install the software on your own machines or shared servers at the university (RCC, SSRC) in order to perform these operations on much more text. \n",
    "\n",
    "For this notebook we will be using the following packages:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#All these packages need to be installed from pip\n",
    "#For NLP\n",
    "import nltk\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "from nltk.parse import stanford\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tree import Tree\n",
    "from nltk.draw.tree import TreeView\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import sklearn\n",
    "\n",
    "import numpy as np #For arrays\n",
    "import pandas #Gives us DataFrames\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "import seaborn #Makes the graphics look nicer\n",
    "import pandas as pd # I don't want to write 'pandas' all the fucking time\n",
    "\n",
    "#Displays the graphs\n",
    "import graphviz #You also need to install the command line graphviz\n",
    "\n",
    "#These are from the standard library\n",
    "import os.path\n",
    "import zipfile\n",
    "import subprocess\n",
    "import io\n",
    "import tempfile\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use the [Stanford NLP group](http://nlp.stanford.edu/) programs with nltk on your own machine (you do *not* need to do this for this assignment), it will require a little bit of setup. We are basing these instructions on those provided by nltk, [here](https://github.com/nltk/nltk/wiki/Installing-Third-Party-Software#stanford-tagger-ner-tokenizer-and-parser), but with small changes to work with our notebooks. We also note that lower performance versions of many of the techniques demonstrated here are available natively within nltk (see the updated [nltk book](http://www.nltk.org/book/)).\n",
    "\n",
    "1. Install [Java 1.8+](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html)\n",
    "    + Make sure your `JAVAPATH` is setup if you're on windows\n",
    "2. Download the following zip files from the Stanford NLP group, where DATE is the release date of the files, this will be the value of `stanfordVersion`\n",
    "    + [`stanford-corenlp-full-2016-10-31.zip`](https://stanfordnlp.github.io/CoreNLP/)\n",
    "    + [`stanford-postagger-full-DATE.zip`](http://nlp.stanford.edu/software/tagger.html#Download)\n",
    "    + [`stanford-ner-DATE.zip`](http://nlp.stanford.edu/software/CRF-NER.html#Download)\n",
    "    + [`stanford-parser-full-DATE.zip`](http://nlp.stanford.edu/software/lex-parser.html#Download)\n",
    "3. Unzip the files and place the resulting directories in the same location, this will become `stanfordDir`\n",
    "4. Lookup the version number used by the parser `stanford-parser-VERSION-models.jar` and set to to be `parserVersion`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This is the date at the end of each of the zip files, e.g.\n",
    "#the date in stanford-ner-2016-10-31.zip\n",
    "stanfordVersion = '2016-10-31'\n",
    "\n",
    "#This is the version numbers of the parser models, these\n",
    "#are files in `stanford-parser-full-2016-10-31.zip`, e.g.\n",
    "#stanford-parser-3.7.0-models.jar\n",
    "parserVersion = '3.7.0'\n",
    "\n",
    "#This is where the zip files were unzipped.Make sure to\n",
    "#unzip into directories named after the zip files\n",
    "#Don't just put all the files in `stanford-NLP`\n",
    "try:\n",
    "    # Because the server for this class is like having an air conditioning powered by \n",
    "    # an asthmatic blowing at you with a straw.\n",
    "    stanfordDir = '/home/jmcclellan/stanford-NLP'\n",
    "except:\n",
    "    stanfordDir = '/mnt/efs/resources/shared/stanford-NLP'\n",
    "    \n",
    "\n",
    "#Parser model, there are a few for english and a couple of other languages as well\n",
    "modelName = 'englishPCFG.ser.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now will initialize all the tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up [NER tagger](http://www.nltk.org/api/nltk.tag.html?highlight=stanfordpostagger#nltk.tag.stanford.StanfordNERTagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nerClassifierPath = os.path.join(stanfordDir,'stanford-ner-{}'.format(stanfordVersion), 'classifiers/english.all.3class.distsim.crf.ser.gz')\n",
    "\n",
    "nerJarPath = os.path.join(stanfordDir,'stanford-ner-{}'.format(stanfordVersion), 'stanford-ner.jar')\n",
    "\n",
    "nerTagger = StanfordNERTagger(nerClassifierPath, nerJarPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up [POS Tagger](http://www.nltk.org/api/nltk.tag.html?highlight=stanfordpostagger#nltk.tag.stanford.StanfordPOSTagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "postClassifierPath = os.path.join(stanfordDir, 'stanford-postagger-full-{}'.format(stanfordVersion), 'models/english-bidirectional-distsim.tagger')\n",
    "\n",
    "postJarPath = os.path.join(stanfordDir,'stanford-postagger-full-{}'.format(stanfordVersion), 'stanford-postagger.jar')\n",
    "\n",
    "postTagger = StanfordPOSTagger(postClassifierPath, postJarPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up [Parser](http://www.nltk.org/api/nltk.parse.html?highlight=stanfordparser#module-nltk.parse.stanford)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parserJarPath = os.path.join(stanfordDir, 'stanford-parser-full-{}'.format(stanfordVersion), 'stanford-parser.jar')\n",
    "\n",
    "parserModelsPath = os.path.join(stanfordDir, 'stanford-parser-full-{}'.format(stanfordVersion), 'stanford-parser-{}-models.jar'.format(parserVersion))\n",
    "\n",
    "modelPath = os.path.join(stanfordDir, 'stanford-parser-full-{}'.format(stanfordVersion), modelName)\n",
    "\n",
    "#The model files are stored in the jar, we need to extract them for nltk to use\n",
    "if not os.path.isfile(modelPath):\n",
    "    with zipfile.ZipFile(parserModelsPath) as zf:\n",
    "        with open(modelPath, 'wb') as f:\n",
    "            f.write(zf.read('edu/stanford/nlp/models/lexparser/{}'.format(modelName)))\n",
    "\n",
    "parser = stanford.StanfordParser(parserJarPath, parserModelsPath, modelPath)\n",
    "\n",
    "depParser = stanford.StanfordDependencyParser(parserJarPath, parserModelsPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Information Extraction is a module packaged within the Stanford Core NLP package, but it is not yet supported by `nltk`. As a result, we will be defining our own function that runs the Stanford Core NLP java code right here. For other projects, it is often useful to use Java or other programs (in C, C++) within a python workflow, and this is an example. `openIE()` takes in a string or list of strings and then produces as output all the subject, verb, object (SVO) triples Stanford Corenlp can find, as a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Watch out, this will very rarely raise an error since it trusts stanford-corenlp \n",
    "def openIE(target):\n",
    "    if isinstance(target, list):\n",
    "        target = '\\n'.join(target)\n",
    "    #setup the java targets\n",
    "    coreDir = '{}/stanford-corenlp-full-{}'.format(stanfordDir, stanfordVersion)\n",
    "    cp = '{0}/stanford-corenlp-{1}.jar:{0}/stanford-corenlp-{1}-models.jar:CoreNLP-to-HTML.xsl:slf4j-api.jar:slf4j-simple.jar'.format(coreDir, parserVersion)\n",
    "    with tempfile.NamedTemporaryFile(mode = 'w', delete = False) as f:\n",
    "        #Core nlp requires a files, so we will make a temp one to pass to it\n",
    "        #This file should be deleted by the OS soon after it has been used\n",
    "        f.write(target)\n",
    "        f.seek(0)\n",
    "        print(\"Starting OpenIE run\")\n",
    "        #If you know what these options do then you should mess with them on your own machine and not the shared server\n",
    "        sp = subprocess.run(['java', '-mx2g', '-cp', cp, 'edu.stanford.nlp.naturalli.OpenIE', '-threads', '1', f.name], stdout = subprocess.PIPE, stderr = subprocess.PIPE)\n",
    "        #Live stderr is non-trivial so this is the best we can do\n",
    "        print(sp.stderr.decode('utf-8'))\n",
    "        retSting = sp.stdout.decode('utf-8')\n",
    "    #Making the DataFrame, again having to pass a fake file, yay POSIX I guess\n",
    "    with io.StringIO(retSting) as f:\n",
    "        df = pandas.read_csv(f, delimiter = '\\t', names =['certainty', 'subject', 'verb', 'object'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will illustrate these tools on some *very* short examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I saw the elephant in my pajamas.\n",
      "The quick brown fox jumped over the lazy dog.\n",
      "While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.\n",
      "Trayvon Benjamin Martin was an African American from Miami Gardens, Florida, who, at 17 years old, was fatally shot by George Zimmerman, a neighborhood watch volunteer, in Sanford, Florida.\n",
      "Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo\n"
     ]
    }
   ],
   "source": [
    "text = ['I saw the elephant in my pajamas.', 'The quick brown fox jumped over the lazy dog.', 'While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.', 'Trayvon Benjamin Martin was an African American from Miami Gardens, Florida, who, at 17 years old, was fatally shot by George Zimmerman, a neighborhood watch volunteer, in Sanford, Florida.', 'Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo']\n",
    "tokenized_text = [word_tokenize(t) for t in text]\n",
    "print('\\n'.join(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-Speech (POS) tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In POS tagging, we classify each word by its semantic role in a sentence. The Stanford POS tagger uses the [Penn Treebank tag set]('http://repository.upenn.edu/cgi/viewcontent.cgi?article=1603&context=cis_reports') to POS tag words from input sentences. As discussed in the second assignment, this is a relatively precise tagset, which allows more informative tags, and also more opportunities to err :-).\n",
    "\n",
    "|#. |Tag |Description |\n",
    "|---|----|------------|\n",
    "|1.\t|CC\t|Coordinating conjunction\n",
    "|2.\t|CD\t|Cardinal number\n",
    "|3.\t|DT\t|Determiner\n",
    "|4.\t|EX\t|Existential there\n",
    "|5.\t|FW\t|Foreign word\n",
    "|6.\t|IN\t|Preposition or subordinating conjunction\n",
    "|7.\t|JJ\t|Adjective\n",
    "|8.\t|JJR|\tAdjective, comparative\n",
    "|9.\t|JJS|\tAdjective, superlative\n",
    "|10.|\tLS\t|List item marker\n",
    "|11.|\tMD\t|Modal\n",
    "|12.|\tNN\t|Noun, singular or mass\n",
    "|13.|\tNNS\t|Noun, plural\n",
    "|14.|\tNNP\t|Proper noun, singular\n",
    "|15.|\tNNPS|\tProper noun, plural\n",
    "|16.|\tPDT\t|Predeterminer\n",
    "|17.|\tPOS\t|Possessive ending\n",
    "|18.|\tPRP\t|Personal pronoun\n",
    "|19.|\tPRP\\$|\tPossessive pronoun\n",
    "|20.|\tRB\t|Adverb\n",
    "|21.|\tRBR\t|Adverb, comparative\n",
    "|22.|\tRBS\t|Adverb, superlative\n",
    "|23.|\tRP\t|Particle\n",
    "|24.|\tSYM\t|Symbol\n",
    "|25.|\tTO\t|to\n",
    "|26.|\tUH\t|Interjection\n",
    "|27.|\tVB\t|Verb, base form\n",
    "|28.|\tVBD\t|Verb, past tense\n",
    "|29.|\tVBG\t|Verb, gerund or present participle\n",
    "|30.|\tVBN\t|Verb, past participle\n",
    "|31.|\tVBP\t|Verb, non-3rd person singular present\n",
    "|32.|\tVBZ\t|Verb, 3rd person singular present\n",
    "|33.|\tWDT\t|Wh-determiner\n",
    "|34.|\tWP\t|Wh-pronoun\n",
    "|35.|\tWP$\t|Possessive wh-pronoun\n",
    "|36.|\tWRB\t|Wh-adverb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Needed Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_pos(target, POS_sents):\n",
    "    '''\n",
    "    Target should be a string that is the part of speech of interest.\n",
    "    \n",
    "    POS_sents should be the column of a dataframe (series) corresponding to processed sentences that contain \n",
    "    the word/POS tuples.\n",
    "    '''\n",
    "    countTarget = target\n",
    "    targetCounts = {}\n",
    "    for entry in POS_sents:\n",
    "        for sentence in entry:\n",
    "            for ent, kind in sentence:\n",
    "                if kind != countTarget:\n",
    "                    continue\n",
    "                elif ent in targetCounts:\n",
    "                    targetCounts[ent] += 1\n",
    "                else:\n",
    "                    targetCounts[ent] = 1\n",
    "    sortedTargets = sorted(targetCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "    \n",
    "    return(sortedTargets)\n",
    "\n",
    "\n",
    "def count_mod_adj(target, word, POS_sents):\n",
    "    NResults = set()\n",
    "    for entry in POS_sents:\n",
    "        for sentence in entry:\n",
    "            for (ent1, kind1),(ent2,kind2) in zip(sentence[:-1], sentence[1:]):\n",
    "                if (kind1,ent2.lower())==(target,word):\n",
    "                    NResults.add(ent1)\n",
    "                else:\n",
    "                    continue\n",
    "    return(NResults)\n",
    "\n",
    "\n",
    "def count_entities(classified_sents):\n",
    "    '''\n",
    "    '''\n",
    "    entityCounts = {}\n",
    "    for entry in classified_sents:\n",
    "        for sentence in entry:\n",
    "            for ent, kind in sentence:\n",
    "                if ent in entityCounts:\n",
    "                    entityCounts[ent] += 1\n",
    "                else:\n",
    "                    entityCounts[ent] = 1\n",
    "    sortedEntities = sorted(entityCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "    return(sortedEntities)\n",
    "\n",
    "\n",
    "def count_nonobjs(classified_sents):\n",
    "    '''\n",
    "    '''\n",
    "    nonObjCounts = {}\n",
    "    for entry in classified_sents:\n",
    "        for sentence in entry:\n",
    "            for ent, kind in sentence:\n",
    "                if kind == 'O':\n",
    "                    continue\n",
    "                elif ent in nonObjCounts:\n",
    "                    nonObjCounts[ent] += 1\n",
    "                else:\n",
    "                    nonObjCounts[ent] = 1\n",
    "    sortedNonObj = sorted(nonObjCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "    \n",
    "    return(sortedNonObj)\n",
    "\n",
    "\n",
    "def count_orgs(classified_sents):\n",
    "    '''\n",
    "    '''\n",
    "    OrgCounts = {}\n",
    "    for entry in classified_sents:\n",
    "        for sentence in entry:\n",
    "            for ent, kind in sentence:\n",
    "                if kind != 'ORGANIZATION':\n",
    "                    continue\n",
    "                elif ent in OrgCounts:\n",
    "                    OrgCounts[ent] += 1\n",
    "                else:\n",
    "                    OrgCounts[ent] = 1\n",
    "    sortedOrgs = sorted(OrgCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "    return(sortedOrgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def treesRelation(parsetrees, relationType, *targets):\n",
    "    return_list = []\n",
    "    for tree in parsetrees:\n",
    "        return_list += treeRelation(tree, relationType, *targets)\n",
    "    return(return_list)\n",
    "\n",
    "def treeRelation(parsetree, relationType, *targets):\n",
    "    if isinstance(parsetree, list):\n",
    "        parsetree = parsetree[0]\n",
    "    if set(targets) & set(parsetree.leaves()) != set(targets):\n",
    "        return []\n",
    "    else:\n",
    "        retList = []\n",
    "        for subT in parsetree.subtrees():\n",
    "            if subT.label() == relationType:\n",
    "                if set(targets) & set(subT.leaves()) == set(targets):\n",
    "                    retList.append([(subT.label(), ' '.join(subT.leaves()))])\n",
    "    return retList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def treeSubRelation(parsetree, relationTypeScope, relationTypeTarget, *targets):\n",
    "    if isinstance(parsetree, list):\n",
    "        parsetree = parsetree[0]\n",
    "    if set(targets) & set(parsetree.leaves()) != set(targets):\n",
    "        return []\n",
    "    else:\n",
    "        retSet = set()\n",
    "        for subT in parsetree.subtrees():\n",
    "            if set(targets) & set(subT.leaves()) == set(targets):\n",
    "                if subT.label() == relationTypeScope:\n",
    "                    for subsub in subT.subtrees():\n",
    "                        if subsub.label()==relationTypeTarget:\n",
    "                            retSet.add(' '.join(subsub.leaves()))\n",
    "    return retSet\n",
    "\n",
    "def treesSubRelation(parsetrees, relationTypeScope, relationTypeTarget, *targets):\n",
    "    return_set = set()\n",
    "    for tree in parsetrees:\n",
    "        return_set = return_set.union(treeSubRelation(tree, relationTypeScope, relationTypeTarget, *targets))\n",
    "    return(return_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Your turn (POS TAGGING)*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, perform POS tagging on a meaningful (but modest) subset of a corpus associated with your final project. Examine the list of words associated with at least three different parts of speech. Consider conditional associations (e.g., adjectives associated with nouns or adverbs with verbs of interest). What do these distributions suggest about your corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we will be looking at some of the top posts of all time from the subreddit /r/changemyview. Let's first load up the data and select a reasonable sample to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cmv_df = pd.read_pickle('data/cmv_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's specifically look at the sufficiently long (over 500 characters) first-tier comments (direct replies) to a submission from /r/changemyview having to do with Brexit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I voted remain. I have just woken up to the Leave victory. My concern is that today the pound has dropped 13% in value. We have no other trade agreements outside of the EU, meaning we will likely still have to trade with the EU under EU regulations. We haven't lost dependence on the EU but we have lost our say in those events. Cameron (who I'm not a fan of) has stepped down, meaning we could potentially end up with the second unelected prime minister in my lifetime. I'm very worried right now and would appreciate some help to see the bright side of this, preferably about some issue other than immigration because I feel that the issue has been so inflated into fearmongering that it's not likely to feel reassuring.\\n\\nI've been researching the referendum for a while and my understanding of the subject is that a lot of people have chosen to vote leave because we were promised a) tighter immigration and b) money set aside for our priorities, e.g. the NHS. Immigration I don't think is being dealt with well since it's been presented as racism vs overidealism. The NHS funding promise was U-turned this morning within minutes of the results. It feels a little like the public have been misled, fearmongered and undereducated on the issue against their best interests.\\n\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the original submission text\n",
    "cmv_df.loc['d4m6i72'].sub_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get comments that only pertain to our submission of interest\n",
    "cmv_df = cmv_df[cmv_df['sub_text'] == cmv_df.loc['d4m6i72'].sub_text]\n",
    "\n",
    "# Filter comments that only have over 500 characters\n",
    "cmv_df['com_char_len'] = cmv_df['com_text'].str.len()\n",
    "cmv_df = cmv_df[cmv_df['com_char_len'] > 500]\n",
    "\n",
    "# Sort from longest to shortest\n",
    "cmv_df = cmv_df.sort_values('com_char_len', ascending = False)\n",
    "\n",
    "len(cmv_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub_text</th>\n",
       "      <th>com_text</th>\n",
       "      <th>com_delta_received</th>\n",
       "      <th>com_delta_from_op</th>\n",
       "      <th>com_upvotes</th>\n",
       "      <th>com_char_len</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>com_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>d4m6vtt</th>\n",
       "      <td>I voted remain. I have just woken up to the Le...</td>\n",
       "      <td>&gt;today the pound has dropped 13% in value.\\n\\n...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>215</td>\n",
       "      <td>3342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d4m7te0</th>\n",
       "      <td>I voted remain. I have just woken up to the Le...</td>\n",
       "      <td>The USSR was the largest example, yes, and it ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>2320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d4m8uat</th>\n",
       "      <td>I voted remain. I have just woken up to the Le...</td>\n",
       "      <td>&gt; They literally covered the planet in trade,\\...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d4m7n52</th>\n",
       "      <td>I voted remain. I have just woken up to the Le...</td>\n",
       "      <td>Prior to the 15th century, Britain was a land ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>1624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d4mei48</th>\n",
       "      <td>I voted remain. I have just woken up to the Le...</td>\n",
       "      <td>I voted Remain, but I'll try anyway.\\n\\nThe Eu...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d4m4yf7</th>\n",
       "      <td>I voted remain. I have just woken up to the Le...</td>\n",
       "      <td>Two words: Greek Debt.\\n\\nThe Greek debt situa...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>442</td>\n",
       "      <td>1245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d4n8dsb</th>\n",
       "      <td>I voted remain. I have just woken up to the Le...</td>\n",
       "      <td>The UK has been taken to its knees over the pa...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d4m69cy</th>\n",
       "      <td>I voted remain. I have just woken up to the Le...</td>\n",
       "      <td>&gt; I mean, we got pissed when you guys taxed ou...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d4n6yep</th>\n",
       "      <td>I voted remain. I have just woken up to the Le...</td>\n",
       "      <td>An empire like Britain in the past found resou...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d4mvt5k</th>\n",
       "      <td>I voted remain. I have just woken up to the Le...</td>\n",
       "      <td>The markets are going crazy because of uncerta...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d4m9zqn</th>\n",
       "      <td>I voted remain. I have just woken up to the Le...</td>\n",
       "      <td>&gt; My concern is that today the pound has dropp...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>20</td>\n",
       "      <td>806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d4o5dbs</th>\n",
       "      <td>I voted remain. I have just woken up to the Le...</td>\n",
       "      <td>The UK will have to get new trade deals with f...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d4nb8e3</th>\n",
       "      <td>I voted remain. I have just woken up to the Le...</td>\n",
       "      <td>It's  important to remember that the EU is not...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d4m1w8k</th>\n",
       "      <td>I voted remain. I have just woken up to the Le...</td>\n",
       "      <td>From someone who voted to Remain: We'll live. ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>56</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d4m9s1e</th>\n",
       "      <td>I voted remain. I have just woken up to the Le...</td>\n",
       "      <td>You need to remember the lost value in the pou...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d4mz5n2</th>\n",
       "      <td>I voted remain. I have just woken up to the Le...</td>\n",
       "      <td>Both sides of the argument are exagerating  th...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d4m8zqk</th>\n",
       "      <td>I voted remain. I have just woken up to the Le...</td>\n",
       "      <td>I know its scary but this is one of those beli...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d4m22eo</th>\n",
       "      <td>I voted remain. I have just woken up to the Le...</td>\n",
       "      <td>It could be argued that UK had a negative role...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>333</td>\n",
       "      <td>651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d4m6p5f</th>\n",
       "      <td>I voted remain. I have just woken up to the Le...</td>\n",
       "      <td>- Look at what the roman empire did! It's the ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d4m26m6</th>\n",
       "      <td>I voted remain. I have just woken up to the Le...</td>\n",
       "      <td>Britain will likely lose a good deal of it's b...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  sub_text  \\\n",
       "com_id                                                       \n",
       "d4m6vtt  I voted remain. I have just woken up to the Le...   \n",
       "d4m7te0  I voted remain. I have just woken up to the Le...   \n",
       "d4m8uat  I voted remain. I have just woken up to the Le...   \n",
       "d4m7n52  I voted remain. I have just woken up to the Le...   \n",
       "d4mei48  I voted remain. I have just woken up to the Le...   \n",
       "d4m4yf7  I voted remain. I have just woken up to the Le...   \n",
       "d4n8dsb  I voted remain. I have just woken up to the Le...   \n",
       "d4m69cy  I voted remain. I have just woken up to the Le...   \n",
       "d4n6yep  I voted remain. I have just woken up to the Le...   \n",
       "d4mvt5k  I voted remain. I have just woken up to the Le...   \n",
       "d4m9zqn  I voted remain. I have just woken up to the Le...   \n",
       "d4o5dbs  I voted remain. I have just woken up to the Le...   \n",
       "d4nb8e3  I voted remain. I have just woken up to the Le...   \n",
       "d4m1w8k  I voted remain. I have just woken up to the Le...   \n",
       "d4m9s1e  I voted remain. I have just woken up to the Le...   \n",
       "d4mz5n2  I voted remain. I have just woken up to the Le...   \n",
       "d4m8zqk  I voted remain. I have just woken up to the Le...   \n",
       "d4m22eo  I voted remain. I have just woken up to the Le...   \n",
       "d4m6p5f  I voted remain. I have just woken up to the Le...   \n",
       "d4m26m6  I voted remain. I have just woken up to the Le...   \n",
       "\n",
       "                                                  com_text com_delta_received  \\\n",
       "com_id                                                                          \n",
       "d4m6vtt  >today the pound has dropped 13% in value.\\n\\n...              False   \n",
       "d4m7te0  The USSR was the largest example, yes, and it ...              False   \n",
       "d4m8uat  > They literally covered the planet in trade,\\...              False   \n",
       "d4m7n52  Prior to the 15th century, Britain was a land ...              False   \n",
       "d4mei48  I voted Remain, but I'll try anyway.\\n\\nThe Eu...              False   \n",
       "d4m4yf7  Two words: Greek Debt.\\n\\nThe Greek debt situa...               True   \n",
       "d4n8dsb  The UK has been taken to its knees over the pa...              False   \n",
       "d4m69cy  > I mean, we got pissed when you guys taxed ou...              False   \n",
       "d4n6yep  An empire like Britain in the past found resou...              False   \n",
       "d4mvt5k  The markets are going crazy because of uncerta...              False   \n",
       "d4m9zqn  > My concern is that today the pound has dropp...              False   \n",
       "d4o5dbs  The UK will have to get new trade deals with f...              False   \n",
       "d4nb8e3  It's  important to remember that the EU is not...              False   \n",
       "d4m1w8k  From someone who voted to Remain: We'll live. ...              False   \n",
       "d4m9s1e  You need to remember the lost value in the pou...              False   \n",
       "d4mz5n2  Both sides of the argument are exagerating  th...              False   \n",
       "d4m8zqk  I know its scary but this is one of those beli...              False   \n",
       "d4m22eo  It could be argued that UK had a negative role...              False   \n",
       "d4m6p5f  - Look at what the roman empire did! It's the ...              False   \n",
       "d4m26m6  Britain will likely lose a good deal of it's b...              False   \n",
       "\n",
       "        com_delta_from_op  com_upvotes  com_char_len  \n",
       "com_id                                                \n",
       "d4m6vtt             False          215          3342  \n",
       "d4m7te0             False            2          2320  \n",
       "d4m8uat             False            1          1656  \n",
       "d4m7n52             False            3          1624  \n",
       "d4mei48             False            1          1381  \n",
       "d4m4yf7             False          442          1245  \n",
       "d4n8dsb             False            1          1044  \n",
       "d4m69cy             False            2           920  \n",
       "d4n6yep             False            1           850  \n",
       "d4mvt5k             False            1           814  \n",
       "d4m9zqn             False           20           806  \n",
       "d4o5dbs             False            1           751  \n",
       "d4nb8e3             False            1           747  \n",
       "d4m1w8k             False           56           710  \n",
       "d4m9s1e             False            2           697  \n",
       "d4mz5n2             False            1           677  \n",
       "d4m8zqk             False            3           672  \n",
       "d4m22eo             False          333           651  \n",
       "d4m6p5f             False           12           631  \n",
       "d4m26m6             False            3           569  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do whatever pre-processing we need to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cmv_df['sentences'] = cmv_df['com_text'].apply(lambda x: [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub_text</th>\n",
       "      <th>com_text</th>\n",
       "      <th>com_delta_received</th>\n",
       "      <th>com_delta_from_op</th>\n",
       "      <th>com_upvotes</th>\n",
       "      <th>com_char_len</th>\n",
       "      <th>sentences</th>\n",
       "      <th>POS_sents</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>com_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>d4m6vtt</th>\n",
       "      <td>I voted remain. I have just woken up to the Le...</td>\n",
       "      <td>&gt;today the pound has dropped 13% in value.\\n\\n...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>215</td>\n",
       "      <td>3342</td>\n",
       "      <td>[[&gt;, today, the, pound, has, dropped, 13, %, i...</td>\n",
       "      <td>[[(&gt;, JJR), (today, NN), (the, DT), (pound, NN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d4m7te0</th>\n",
       "      <td>I voted remain. I have just woken up to the Le...</td>\n",
       "      <td>The USSR was the largest example, yes, and it ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>2320</td>\n",
       "      <td>[[The, USSR, was, the, largest, example, ,, ye...</td>\n",
       "      <td>[[(The, DT), (USSR, NNP), (was, VBD), (the, DT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d4m8uat</th>\n",
       "      <td>I voted remain. I have just woken up to the Le...</td>\n",
       "      <td>&gt; They literally covered the planet in trade,\\...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1656</td>\n",
       "      <td>[[&gt;, They, literally, covered, the, planet, in...</td>\n",
       "      <td>[[(&gt;, JJR), (They, PRP), (literally, RB), (cov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d4m7n52</th>\n",
       "      <td>I voted remain. I have just woken up to the Le...</td>\n",
       "      <td>Prior to the 15th century, Britain was a land ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>1624</td>\n",
       "      <td>[[Prior, to, the, 15th, century, ,, Britain, w...</td>\n",
       "      <td>[[(Prior, RB), (to, TO), (the, DT), (15th, JJ)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d4mei48</th>\n",
       "      <td>I voted remain. I have just woken up to the Le...</td>\n",
       "      <td>I voted Remain, but I'll try anyway.\\n\\nThe Eu...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1381</td>\n",
       "      <td>[[I, voted, Remain, ,, but, I, 'll, try, anywa...</td>\n",
       "      <td>[[(I, PRP), (voted, VBD), (Remain, NNP), (,, ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d4m4yf7</th>\n",
       "      <td>I voted remain. I have just woken up to the Le...</td>\n",
       "      <td>Two words: Greek Debt.\\n\\nThe Greek debt situa...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>442</td>\n",
       "      <td>1245</td>\n",
       "      <td>[[Two, words, :, Greek, Debt, .], [The, Greek,...</td>\n",
       "      <td>[[(Two, CD), (words, NNS), (:, :), (Greek, JJ)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d4n8dsb</th>\n",
       "      <td>I voted remain. I have just woken up to the Le...</td>\n",
       "      <td>The UK has been taken to its knees over the pa...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1044</td>\n",
       "      <td>[[The, UK, has, been, taken, to, its, knees, o...</td>\n",
       "      <td>[[(The, DT), (UK, NNP), (has, VBZ), (been, VBN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d4m69cy</th>\n",
       "      <td>I voted remain. I have just woken up to the Le...</td>\n",
       "      <td>&gt; I mean, we got pissed when you guys taxed ou...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>920</td>\n",
       "      <td>[[&gt;, I, mean, ,, we, got, pissed, when, you, g...</td>\n",
       "      <td>[[(&gt;, JJR), (I, PRP), (mean, VBP), (,, ,), (we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d4n6yep</th>\n",
       "      <td>I voted remain. I have just woken up to the Le...</td>\n",
       "      <td>An empire like Britain in the past found resou...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>850</td>\n",
       "      <td>[[An, empire, like, Britain, in, the, past, fo...</td>\n",
       "      <td>[[(An, DT), (empire, NN), (like, IN), (Britain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d4mvt5k</th>\n",
       "      <td>I voted remain. I have just woken up to the Le...</td>\n",
       "      <td>The markets are going crazy because of uncerta...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>814</td>\n",
       "      <td>[[The, markets, are, going, crazy, because, of...</td>\n",
       "      <td>[[(The, DT), (markets, NNS), (are, VBP), (goin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  sub_text  \\\n",
       "com_id                                                       \n",
       "d4m6vtt  I voted remain. I have just woken up to the Le...   \n",
       "d4m7te0  I voted remain. I have just woken up to the Le...   \n",
       "d4m8uat  I voted remain. I have just woken up to the Le...   \n",
       "d4m7n52  I voted remain. I have just woken up to the Le...   \n",
       "d4mei48  I voted remain. I have just woken up to the Le...   \n",
       "d4m4yf7  I voted remain. I have just woken up to the Le...   \n",
       "d4n8dsb  I voted remain. I have just woken up to the Le...   \n",
       "d4m69cy  I voted remain. I have just woken up to the Le...   \n",
       "d4n6yep  I voted remain. I have just woken up to the Le...   \n",
       "d4mvt5k  I voted remain. I have just woken up to the Le...   \n",
       "\n",
       "                                                  com_text com_delta_received  \\\n",
       "com_id                                                                          \n",
       "d4m6vtt  >today the pound has dropped 13% in value.\\n\\n...              False   \n",
       "d4m7te0  The USSR was the largest example, yes, and it ...              False   \n",
       "d4m8uat  > They literally covered the planet in trade,\\...              False   \n",
       "d4m7n52  Prior to the 15th century, Britain was a land ...              False   \n",
       "d4mei48  I voted Remain, but I'll try anyway.\\n\\nThe Eu...              False   \n",
       "d4m4yf7  Two words: Greek Debt.\\n\\nThe Greek debt situa...               True   \n",
       "d4n8dsb  The UK has been taken to its knees over the pa...              False   \n",
       "d4m69cy  > I mean, we got pissed when you guys taxed ou...              False   \n",
       "d4n6yep  An empire like Britain in the past found resou...              False   \n",
       "d4mvt5k  The markets are going crazy because of uncerta...              False   \n",
       "\n",
       "        com_delta_from_op  com_upvotes  com_char_len  \\\n",
       "com_id                                                 \n",
       "d4m6vtt             False          215          3342   \n",
       "d4m7te0             False            2          2320   \n",
       "d4m8uat             False            1          1656   \n",
       "d4m7n52             False            3          1624   \n",
       "d4mei48             False            1          1381   \n",
       "d4m4yf7             False          442          1245   \n",
       "d4n8dsb             False            1          1044   \n",
       "d4m69cy             False            2           920   \n",
       "d4n6yep             False            1           850   \n",
       "d4mvt5k             False            1           814   \n",
       "\n",
       "                                                 sentences  \\\n",
       "com_id                                                       \n",
       "d4m6vtt  [[>, today, the, pound, has, dropped, 13, %, i...   \n",
       "d4m7te0  [[The, USSR, was, the, largest, example, ,, ye...   \n",
       "d4m8uat  [[>, They, literally, covered, the, planet, in...   \n",
       "d4m7n52  [[Prior, to, the, 15th, century, ,, Britain, w...   \n",
       "d4mei48  [[I, voted, Remain, ,, but, I, 'll, try, anywa...   \n",
       "d4m4yf7  [[Two, words, :, Greek, Debt, .], [The, Greek,...   \n",
       "d4n8dsb  [[The, UK, has, been, taken, to, its, knees, o...   \n",
       "d4m69cy  [[>, I, mean, ,, we, got, pissed, when, you, g...   \n",
       "d4n6yep  [[An, empire, like, Britain, in, the, past, fo...   \n",
       "d4mvt5k  [[The, markets, are, going, crazy, because, of...   \n",
       "\n",
       "                                                 POS_sents  \n",
       "com_id                                                      \n",
       "d4m6vtt  [[(>, JJR), (today, NN), (the, DT), (pound, NN...  \n",
       "d4m7te0  [[(The, DT), (USSR, NNP), (was, VBD), (the, DT...  \n",
       "d4m8uat  [[(>, JJR), (They, PRP), (literally, RB), (cov...  \n",
       "d4m7n52  [[(Prior, RB), (to, TO), (the, DT), (15th, JJ)...  \n",
       "d4mei48  [[(I, PRP), (voted, VBD), (Remain, NNP), (,, ,...  \n",
       "d4m4yf7  [[(Two, CD), (words, NNS), (:, :), (Greek, JJ)...  \n",
       "d4n8dsb  [[(The, DT), (UK, NNP), (has, VBZ), (been, VBN...  \n",
       "d4m69cy  [[(>, JJR), (I, PRP), (mean, VBP), (,, ,), (we...  \n",
       "d4n6yep  [[(An, DT), (empire, NN), (like, IN), (Britain...  \n",
       "d4mvt5k  [[(The, DT), (markets, NNS), (are, VBP), (goin...  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmv_df['POS_sents'] = cmv_df['sentences'].apply(lambda x: postTagger.tag_sents(x))\n",
    "cmv_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cmv_df['classified_sents'] = cmv_df['sentences'].apply(lambda x: nerTagger.tag_sents(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "com_id\n",
       "d4m6vtt    [[(>, O), (today, O), (the, O), (pound, O), (h...\n",
       "d4m7te0    [[(The, O), (USSR, LOCATION), (was, O), (the, ...\n",
       "d4m8uat    [[(>, O), (They, O), (literally, O), (covered,...\n",
       "d4m7n52    [[(Prior, O), (to, O), (the, O), (15th, O), (c...\n",
       "d4mei48    [[(I, O), (voted, O), (Remain, O), (,, O), (bu...\n",
       "d4m4yf7    [[(Two, O), (words, O), (:, O), (Greek, O), (D...\n",
       "d4n8dsb    [[(The, O), (UK, LOCATION), (has, O), (been, O...\n",
       "d4m69cy    [[(>, O), (I, O), (mean, O), (,, O), (we, O), ...\n",
       "d4n6yep    [[(An, O), (empire, O), (like, O), (Britain, L...\n",
       "d4mvt5k    [[(The, O), (markets, O), (are, O), (going, O)...\n",
       "d4m9zqn    [[(>, O), (My, O), (concern, O), (is, O), (tha...\n",
       "d4o5dbs    [[(The, O), (UK, LOCATION), (will, O), (have, ...\n",
       "d4nb8e3    [[(It, O), ('s, O), (important, O), (to, O), (...\n",
       "d4m1w8k    [[(From, O), (someone, O), (who, O), (voted, O...\n",
       "d4m9s1e    [[(You, O), (need, O), (to, O), (remember, O),...\n",
       "d4mz5n2    [[(Both, O), (sides, O), (of, O), (the, O), (a...\n",
       "d4m8zqk    [[(I, O), (know, O), (its, O), (scary, O), (bu...\n",
       "d4m22eo    [[(It, O), (could, O), (be, O), (argued, O), (...\n",
       "d4m6p5f    [[(-, O), (Look, O), (at, O), (what, O), (the,...\n",
       "d4m26m6    [[(Britain, LOCATION), (will, O), (likely, O),...\n",
       "Name: classified_sents, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmv_df['classified_sents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cmv_df['word_tokens'] = cmv_df['com_text'].apply(lambda x: word_tokenize(x))\n",
    "cmv_df['each_sentence_is_list'] = cmv_df['com_text'].apply(lambda x: sent_tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Pre-processing done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at counts of nouns, verbs, and adjectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "com_nouns = count_pos('NN', cmv_df['POS_sents'])\n",
    "com_verbs = count_pos('VB', cmv_df['POS_sents'])\n",
    "com_adj = count_pos('JJ', cmv_df['POS_sents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('be', 26),\n",
       " ('have', 12),\n",
       " ('make', 9),\n",
       " ('get', 7),\n",
       " ('happen', 6),\n",
       " ('trade', 6),\n",
       " ('try', 4),\n",
       " ('say', 3),\n",
       " ('do', 3),\n",
       " ('hold', 3),\n",
       " ('end', 3),\n",
       " ('create', 3),\n",
       " ('Leave', 3),\n",
       " ('think', 3),\n",
       " ('give', 2),\n",
       " ('know', 2),\n",
       " ('afford', 2),\n",
       " ('defeat', 2),\n",
       " ('want', 2),\n",
       " ('use', 2),\n",
       " ('lose', 2),\n",
       " ('help', 2),\n",
       " ('remember', 2),\n",
       " ('see', 2),\n",
       " ('pull', 2),\n",
       " ('break', 2),\n",
       " ('go', 2),\n",
       " ('buy', 2),\n",
       " ('change', 2),\n",
       " ('conquer', 2),\n",
       " ('negotiate', 2),\n",
       " ('leave', 2),\n",
       " ('pay', 2),\n",
       " ('remain', 2),\n",
       " ('recover', 1),\n",
       " ('orient', 1),\n",
       " ('ensure', 1),\n",
       " ('realise', 1),\n",
       " ('join', 1),\n",
       " ('hurt', 1),\n",
       " ('follow', 1),\n",
       " ('rise', 1),\n",
       " ('*without*', 1),\n",
       " ('grow', 1),\n",
       " ('continue', 1),\n",
       " ('cover', 1),\n",
       " ('lead', 1),\n",
       " ('stop', 1),\n",
       " ('matter', 1),\n",
       " ('forge', 1),\n",
       " ('exist', 1),\n",
       " ('look', 1),\n",
       " ('stabilize', 1),\n",
       " ('trigger', 1),\n",
       " ('gain', 1),\n",
       " ('stay', 1),\n",
       " ('deter', 1),\n",
       " ('act', 1),\n",
       " ('win', 1),\n",
       " ('consider', 1),\n",
       " ('sell', 1),\n",
       " ('achieve', 1),\n",
       " ('notice', 1),\n",
       " ('fall', 1),\n",
       " ('succeed', 1),\n",
       " ('declare', 1),\n",
       " ('excuse', 1),\n",
       " ('Look', 1),\n",
       " ('claim', 1),\n",
       " ('believe', 1),\n",
       " ('Remain', 1),\n",
       " ('set', 1),\n",
       " ('approve', 1),\n",
       " ('deal', 1),\n",
       " ('benefit', 1),\n",
       " ('reform', 1),\n",
       " ('ridicule', 1),\n",
       " ('bargain', 1),\n",
       " ('spend', 1),\n",
       " ('wake', 1),\n",
       " ('rely', 1),\n",
       " ('throw', 1),\n",
       " ('play', 1),\n",
       " ('apply', 1),\n",
       " ('host', 1),\n",
       " ('wrangle', 1),\n",
       " ('live', 1),\n",
       " ('boost', 1),\n",
       " ('*against*', 1),\n",
       " ('agree', 1),\n",
       " ('hand', 1),\n",
       " ('suck', 1),\n",
       " ('convince', 1),\n",
       " ('refocus', 1),\n",
       " ('implode', 1),\n",
       " ('name', 1),\n",
       " ('penalize', 1),\n",
       " ('steal', 1)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com_verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the example, 'be' is the top verb. However, we already can see that 'trade' is up there with 6 occurrences, which we might expect given the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('trade', 18),\n",
       " ('world', 16),\n",
       " ('country', 13),\n",
       " (')', 10),\n",
       " ('economy', 9),\n",
       " ('time', 8),\n",
       " ('currency', 8),\n",
       " ('lot', 8),\n",
       " ('value', 7),\n",
       " ('navy', 6),\n",
       " ('place', 6),\n",
       " ('bullshit', 6),\n",
       " ('power', 6),\n",
       " ('pound', 6),\n",
       " ('war', 5),\n",
       " ('%', 5),\n",
       " ('century', 5),\n",
       " ('control', 5),\n",
       " ('point', 5),\n",
       " ('revolution', 5),\n",
       " ('exchange', 5),\n",
       " ('course', 5),\n",
       " ('future', 5),\n",
       " ('thing', 4),\n",
       " ('issue', 4),\n",
       " ('reason', 4),\n",
       " ('example', 4),\n",
       " ('empire', 4),\n",
       " ('fact', 4),\n",
       " ('way', 4),\n",
       " ('immigration', 4),\n",
       " ('independence', 4),\n",
       " ('member', 4),\n",
       " ('trading', 3),\n",
       " ('bit', 3),\n",
       " ('Look', 3),\n",
       " ('end', 3),\n",
       " ('run', 3),\n",
       " ('(', 3),\n",
       " ('something', 3),\n",
       " ('freedom', 3),\n",
       " ('movement', 3),\n",
       " ('rate', 3),\n",
       " ('market', 3),\n",
       " ('part', 3),\n",
       " ('term', 3),\n",
       " ('change', 3),\n",
       " ('money', 3),\n",
       " ('gold', 2),\n",
       " ('history', 2),\n",
       " ('access', 2),\n",
       " ('level', 2),\n",
       " ('opportunity', 2),\n",
       " ('state', 2),\n",
       " ('sea', 2),\n",
       " ('number', 2),\n",
       " ('etc', 2),\n",
       " ('price', 2),\n",
       " ('stock', 2),\n",
       " ('truck', 2),\n",
       " ('union', 2),\n",
       " ('unemployment', 2),\n",
       " ('stuff', 2),\n",
       " ('advantage', 2),\n",
       " ('block', 2),\n",
       " ('dogshit', 2),\n",
       " ('past', 2),\n",
       " ('year', 2),\n",
       " ('opinion', 2),\n",
       " ('period', 2),\n",
       " ('labor', 2),\n",
       " ('dump', 2),\n",
       " ('problem', 2),\n",
       " ('kennel', 2),\n",
       " ('earth', 2),\n",
       " ('agreement', 2),\n",
       " ('effect', 2),\n",
       " ('shit', 2),\n",
       " ('debt', 2),\n",
       " ('co-operation', 2),\n",
       " ('body', 2),\n",
       " ('tariff', 2),\n",
       " ('austerity', 2),\n",
       " ('day', 2),\n",
       " ('scenario', 2),\n",
       " ('process', 2),\n",
       " ('deal', 2),\n",
       " ('idea', 2),\n",
       " ('government', 2),\n",
       " ('regard', 2),\n",
       " ('growth', 2),\n",
       " ('situation', 2),\n",
       " ('today', 2),\n",
       " ('legislation', 2),\n",
       " ('choice', 2),\n",
       " ('collapse', 2),\n",
       " ('population', 2),\n",
       " ('USSR', 2),\n",
       " ('uncertainty', 2),\n",
       " ('sovereignty', 2),\n",
       " ('**Romania', 1),\n",
       " ('status', 1),\n",
       " ('rest', 1),\n",
       " ('otherhand', 1),\n",
       " ('cooperation', 1),\n",
       " ('minister', 1),\n",
       " ('island', 1),\n",
       " ('mess', 1),\n",
       " ('unto', 1),\n",
       " ('aspect', 1),\n",
       " ('violently**', 1),\n",
       " ('order', 1),\n",
       " ('ability', 1),\n",
       " ('minimizer', 1),\n",
       " ('colonialism', 1),\n",
       " ('truth', 1),\n",
       " ('hold', 1),\n",
       " ('failure', 1),\n",
       " ('risk', 1),\n",
       " ('humility', 1),\n",
       " ('scheme', 1),\n",
       " ('kind', 1),\n",
       " ('will', 1),\n",
       " ('guess', 1),\n",
       " ('negotiating', 1),\n",
       " ('argument', 1),\n",
       " ('month', 1),\n",
       " ('crisis', 1),\n",
       " ('entry', 1),\n",
       " ('AKA', 1),\n",
       " ('hardship', 1),\n",
       " ('backbone', 1),\n",
       " ('whole', 1),\n",
       " ('case', 1),\n",
       " ('isolation', 1),\n",
       " ('tandem', 1),\n",
       " ('sort', 1),\n",
       " ('notch', 1),\n",
       " ('Sort', 1),\n",
       " ('fuck', 1),\n",
       " ('life', 1),\n",
       " ('expansion', 1),\n",
       " ('drop', 1),\n",
       " ('meh', 1),\n",
       " ('extent', 1),\n",
       " ('mediterania', 1),\n",
       " ('finance', 1),\n",
       " ('europe', 1),\n",
       " ('blip', 1),\n",
       " ('gambling', 1),\n",
       " ('face', 1),\n",
       " ('domino', 1),\n",
       " ('TP', 1),\n",
       " ('position', 1),\n",
       " ('passport', 1),\n",
       " ('imperialism', 1),\n",
       " ('difference', 1),\n",
       " ('chunk', 1),\n",
       " ('law', 1),\n",
       " ('parent', 1),\n",
       " ('Debt', 1),\n",
       " ('whomever', 1),\n",
       " ('standing', 1),\n",
       " ('program', 1),\n",
       " ('rule', 1),\n",
       " ('recession', 1),\n",
       " ('opposition', 1),\n",
       " ('WW2', 1),\n",
       " ('ass', 1),\n",
       " ('parallel', 1),\n",
       " ('armada', 1),\n",
       " ('environment', 1),\n",
       " ('satellite', 1),\n",
       " ('concern', 1),\n",
       " ('spot', 1),\n",
       " ('payment', 1),\n",
       " ('neo-liberalism', 1),\n",
       " ('kingdom', 1),\n",
       " ('rug', 1),\n",
       " ('industrialization', 1),\n",
       " ('fit', 1),\n",
       " ('laziness', 1),\n",
       " ('influx', 1),\n",
       " ('fan', 1),\n",
       " ('boost', 1),\n",
       " ('ALL', 1),\n",
       " ('everything', 1),\n",
       " ('share', 1),\n",
       " ('boarder', 1),\n",
       " ('investing', 1),\n",
       " ('import', 1),\n",
       " ('deficit', 1),\n",
       " ('middle', 1),\n",
       " ('leave', 1),\n",
       " ('shitter', 1),\n",
       " ('shot', 1),\n",
       " ('labour', 1),\n",
       " ('location', 1),\n",
       " ('trouble', 1),\n",
       " ('regime', 1),\n",
       " ('*result*', 1),\n",
       " ('balance', 1),\n",
       " ('leaving', 1),\n",
       " ('statement', 1),\n",
       " ('conquest', 1),\n",
       " ('Momentum', 1),\n",
       " ('bargaining', 1),\n",
       " ('thinking', 1),\n",
       " ('Nothing', 1),\n",
       " ('fun', 1),\n",
       " ('evil', 1),\n",
       " ('say', 1),\n",
       " ('head-start', 1),\n",
       " ('commerce', 1),\n",
       " ('ideology', 1),\n",
       " ('safety', 1),\n",
       " ('lifetime', 1),\n",
       " ('anyone', 1),\n",
       " ('someone', 1),\n",
       " ('elite', 1),\n",
       " ('exit', 1),\n",
       " ('Money', 1),\n",
       " ('bureaucracy', 1),\n",
       " ('some/most', 1),\n",
       " ('estate', 1),\n",
       " ('endeavor', 1),\n",
       " ('quo', 1),\n",
       " ('*American*', 1),\n",
       " ('Quote', 1),\n",
       " ('mistake', 1),\n",
       " ('flame', 1),\n",
       " ('treaty', 1),\n",
       " ('Decolonization', 1),\n",
       " ('care', 1),\n",
       " ('role', 1),\n",
       " ('while', 1),\n",
       " ('resistance', 1),\n",
       " ('reform', 1),\n",
       " ('job', 1),\n",
       " ('fluctuation', 1),\n",
       " ('possibility', 1),\n",
       " ('prosperity', 1),\n",
       " ('call', 1),\n",
       " ('debate', 1),\n",
       " ('amount', 1),\n",
       " ('essence', 1),\n",
       " ('use', 1),\n",
       " ('map', 1),\n",
       " ('anything', 1),\n",
       " ('destiny', 1),\n",
       " ('violence', 1),\n",
       " ('continuation', 1),\n",
       " ('drain', 1),\n",
       " ('perspective', 1),\n",
       " ('greek', 1),\n",
       " ('TSA', 1),\n",
       " ('visa', 1),\n",
       " ('export', 1),\n",
       " ('feature', 1),\n",
       " ('**not**', 1),\n",
       " ('eu', 1),\n",
       " ('stability', 1),\n",
       " ('prevention', 1),\n",
       " ('opium', 1),\n",
       " ('isolationism', 1),\n",
       " ('entity', 1),\n",
       " ('cargo', 1),\n",
       " ('glory', 1),\n",
       " ('brain', 1),\n",
       " ('phase', 1),\n",
       " ('restriction', 1),\n",
       " ('system', 1),\n",
       " ('effort', 1),\n",
       " ('land', 1),\n",
       " ('majority', 1),\n",
       " ('self-determination', 1),\n",
       " ('centre', 1),\n",
       " ('philosophy', 1),\n",
       " ('table', 1),\n",
       " ('style', 1),\n",
       " ('measure', 1),\n",
       " ('leeway', 1),\n",
       " ('morning', 1),\n",
       " ('contributor', 1),\n",
       " ('parliament', 1),\n",
       " ('tea', 1),\n",
       " ('border', 1),\n",
       " ('door', 1),\n",
       " ('meaning', 1),\n",
       " ('dog', 1),\n",
       " ('direction', 1),\n",
       " ('center', 1),\n",
       " ('model', 1),\n",
       " ('everybody', 1),\n",
       " ('scare', 1),\n",
       " ('jump', 1),\n",
       " ('pressure', 1),\n",
       " ('lion', 1),\n",
       " ('plenty', 1),\n",
       " ('15th', 1),\n",
       " ('planet', 1)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('other', 18),\n",
       " ('economic', 10),\n",
       " ('same', 8),\n",
       " ('good', 8),\n",
       " ('powerful', 6),\n",
       " ('new', 5),\n",
       " ('great', 5),\n",
       " ('full', 5),\n",
       " ('own', 5),\n",
       " ('long', 5),\n",
       " ('many', 5),\n",
       " ('industrial', 4),\n",
       " ('civil', 4),\n",
       " ('major', 4),\n",
       " ('possible', 4),\n",
       " ('big', 4),\n",
       " ('Greek', 4),\n",
       " ('wrong', 3),\n",
       " ('first', 3),\n",
       " ('different', 3),\n",
       " ('Soviet', 3),\n",
       " ('high', 3),\n",
       " ('bad', 2),\n",
       " ('financial', 2),\n",
       " ('negative', 2),\n",
       " ('instant', 2),\n",
       " ('nonviolent', 2),\n",
       " ('massive', 2),\n",
       " ('last', 2),\n",
       " ('such', 2),\n",
       " ('political', 2),\n",
       " ('theirs', 2),\n",
       " ('single', 2),\n",
       " ('skittish', 2),\n",
       " ('British', 2),\n",
       " ('important', 2),\n",
       " ('strong', 2),\n",
       " ('few', 2),\n",
       " ('free', 2),\n",
       " ('certain', 2),\n",
       " ('past', 2),\n",
       " ('national', 2),\n",
       " ('exact', 2),\n",
       " ('due', 2),\n",
       " ('rational', 2),\n",
       " ('only', 2),\n",
       " ('next', 2),\n",
       " ('former', 2),\n",
       " ('human', 2),\n",
       " ('little', 2),\n",
       " ('sure', 2),\n",
       " ('foreign', 2),\n",
       " ('Mediterranean', 2),\n",
       " ('European', 2),\n",
       " ('common', 2),\n",
       " ('American', 2),\n",
       " ('South', 1),\n",
       " ('yeah', 1),\n",
       " ('temporary', 1),\n",
       " ('unadulterated', 1),\n",
       " ('emotional', 1),\n",
       " ('thin', 1),\n",
       " ('usual', 1),\n",
       " ('close', 1),\n",
       " ('1st', 1),\n",
       " ('imperial', 1),\n",
       " ('unified', 1),\n",
       " ('tough', 1),\n",
       " ('roman', 1),\n",
       " ('Good', 1),\n",
       " ('dominant', 1),\n",
       " ('Southern', 1),\n",
       " ('whole', 1),\n",
       " ('prime', 1),\n",
       " ('hissy', 1),\n",
       " ('serious', 1),\n",
       " ('prohibitive', 1),\n",
       " ('one-party', 1),\n",
       " ('Real', 1),\n",
       " ('external', 1),\n",
       " ('possibile', 1),\n",
       " ('able', 1),\n",
       " ('monetary', 1),\n",
       " ('nation-state', 1),\n",
       " ('true', 1),\n",
       " ('scary', 1),\n",
       " ('devastating', 1),\n",
       " ('sceptic', 1),\n",
       " ('popular', 1),\n",
       " ('unelected', 1),\n",
       " ('21st', 1),\n",
       " ('powerfull', 1),\n",
       " ('short', 1),\n",
       " ('Civil', 1),\n",
       " ('EEA+', 1),\n",
       " ('well-deserved', 1),\n",
       " ('global', 1),\n",
       " ('international', 1),\n",
       " ('Such', 1),\n",
       " ('Communist', 1),\n",
       " ('north', 1),\n",
       " ('unprecedented', 1),\n",
       " ('rhetorical', 1),\n",
       " ('pretty', 1),\n",
       " ('previous', 1),\n",
       " ('unstable', 1),\n",
       " ('main', 1),\n",
       " ('stable', 1),\n",
       " ('unwilling', 1),\n",
       " ('sovereign', 1),\n",
       " ('dramatic', 1),\n",
       " ('structural', 1),\n",
       " ('enough', 1),\n",
       " ('colossal', 1),\n",
       " ('it/had', 1),\n",
       " ('terrible', 1),\n",
       " ('actual', 1),\n",
       " ('near', 1),\n",
       " ('centralized', 1),\n",
       " ('low-commitment', 1),\n",
       " ('advantageous', 1),\n",
       " ('real', 1),\n",
       " ('giant', 1),\n",
       " ('viable', 1),\n",
       " ('irrelevant', 1),\n",
       " ('huge', 1),\n",
       " ('petty', 1),\n",
       " ('blind', 1),\n",
       " ('indebted', 1),\n",
       " ('peaceful', 1),\n",
       " ('lumbering', 1),\n",
       " ('simple', 1),\n",
       " ('impressive', 1),\n",
       " ('magnificent', 1),\n",
       " ('easy', 1),\n",
       " ('favorable', 1),\n",
       " ('weak', 1),\n",
       " ('late', 1),\n",
       " ('specific', 1),\n",
       " ('noncompetitive', 1),\n",
       " ('18th', 1),\n",
       " ('hermit', 1),\n",
       " ('manifest', 1),\n",
       " ('violent', 1),\n",
       " ('democratic', 1),\n",
       " ('mundane', 1),\n",
       " ('normal', 1),\n",
       " ('extensive', 1),\n",
       " ('unlikely', 1),\n",
       " ('crazy', 1),\n",
       " ('Conservative', 1),\n",
       " ('harsh', 1),\n",
       " ('skeptical', 1),\n",
       " ('likely', 1),\n",
       " ('Sorry', 1),\n",
       " ('vast', 1),\n",
       " ('aggressive', 1),\n",
       " ('comparable', 1),\n",
       " ('accountable', 1),\n",
       " ('medium-long', 1),\n",
       " ('fine', 1),\n",
       " ('entrenched', 1),\n",
       " ('second', 1),\n",
       " ('wishful', 1),\n",
       " ('lost', 1),\n",
       " ('risky', 1),\n",
       " ('large', 1),\n",
       " ('smart', 1),\n",
       " ('ancient', 1),\n",
       " ('internal', 1),\n",
       " ('geopolitical', 1),\n",
       " ('unresolved', 1),\n",
       " ('productive', 1),\n",
       " ('modern', 1),\n",
       " ('wake-up', 1),\n",
       " ('long-run', 1),\n",
       " ('15th', 1)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now 'economic' is up there. For now, this only seems to confirm that we are indeed tagging the topic we think we are tagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's look at some conditional associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'noncompetitive', 'powerful', 'strong'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at adjectives that modify the noun 'economy'\n",
    "count_mod_adj(target='JJ', word = 'economy', POS_sents=cmv_df['POS_sents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the list of adjectives above, noncompetitive is only used once, while powerful and strong are used 7 times (1 time as 'powerfull') and twice respectively. This seems to indicate at least, that the responses seem to describe the consequences of a 'Yes' vote (uncompetitive) and the benefits that the EU provides to Britain's economy, the benefits of a 'No' vote (powerful and strong. This doesn't tell us much besides the fact that the responders to this view seem to use similar terms in discussing Brexit that I've seen elsewhere (on the news and what not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'big', 'free', 'new'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at adjectives that modify the noun 'trade'\n",
    "count_mod_adj(target='JJ', word = 'trade', POS_sents=cmv_df['POS_sents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes a lot of sense that 'free'  and 'new' would be adjectives to trade. I could imagine these describing trade under a 'No' vote. I.e. that membership in the EU promotes free trade between members and is helpful for members to take advantage of new trade opportunities within Europe.\n",
    "\n",
    "On the othe hand, the apperance of 'big' is slightly less intuitive, 'big trade' sounds a bit forced, it would be helpful to see the context in which 'big trade' occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'only', 'still'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at adverbs that modify the verb 'trade'\n",
    "count_mod_adj(target='RB', word = 'trade', POS_sents=cmv_df['POS_sents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference with these two adverbs seems a bit difficult. One might infer that 'still' appears since commenters might be saying that Britain could 'still trade' with the EU after a 'Yes' on Brexit. I have trouble interepreting 'only trade', on the other hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Your turn (NER)*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, perform NER on a (modest) subset of your corpus of interest. List all of the different kinds of entities tagged? What does their distribution suggest about the focus of your corpus? For a subset of your corpus, tally at least one type of named entity and calculate the Precision, Recall and F-score for the NER classification just performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most common entities (which are, of course, boring):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 236),\n",
       " (',', 183),\n",
       " ('.', 181),\n",
       " ('to', 116),\n",
       " ('and', 98),\n",
       " ('a', 86),\n",
       " ('of', 79),\n",
       " ('in', 74),\n",
       " ('is', 63),\n",
       " ('that', 58)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortedEntities = count_entities(cmv_df['classified_sents'])\n",
    "sortedEntities[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or those occurring only twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['creating',\n",
       " 'scrambled',\n",
       " 'reform',\n",
       " 'easier',\n",
       " 'instant',\n",
       " 'navies',\n",
       " '2',\n",
       " 'actually',\n",
       " 'last',\n",
       " 'whole',\n",
       " 'state',\n",
       " 'goods',\n",
       " 'political',\n",
       " 'number',\n",
       " 'sure',\n",
       " 'managed',\n",
       " 'Brexit',\n",
       " 'Both',\n",
       " 'truck',\n",
       " 'increasing',\n",
       " 'stuff',\n",
       " 'few',\n",
       " 'considered',\n",
       " 'Russia',\n",
       " 'literally',\n",
       " 'opinion',\n",
       " 'national',\n",
       " 'France',\n",
       " 'rational',\n",
       " 'labor',\n",
       " 'laws',\n",
       " 'lead',\n",
       " 'due',\n",
       " 'am',\n",
       " 'notice',\n",
       " 'year',\n",
       " \"'m\",\n",
       " 'help',\n",
       " 'foreign',\n",
       " 'itself',\n",
       " 'India',\n",
       " 'greater',\n",
       " 'exact',\n",
       " 'Mediterranean',\n",
       " 'Portugal',\n",
       " 'government',\n",
       " 'eggs',\n",
       " 'totally',\n",
       " 'former',\n",
       " 'meaning',\n",
       " 'my',\n",
       " 'uncertainty',\n",
       " 'population',\n",
       " 'bad',\n",
       " 'gold',\n",
       " 'before',\n",
       " 'negative',\n",
       " 'nonviolent',\n",
       " 'needed',\n",
       " 'makes',\n",
       " 'based',\n",
       " 'UKs',\n",
       " 'access',\n",
       " 'why',\n",
       " 'December',\n",
       " 'exports',\n",
       " 'cheaper',\n",
       " 'hands',\n",
       " 'sovereignty',\n",
       " 'Meanwhile',\n",
       " '1945',\n",
       " 'forced',\n",
       " 'afford',\n",
       " 'stealing',\n",
       " 'pull',\n",
       " 'defeat',\n",
       " 'unemployment',\n",
       " 'advantage',\n",
       " 'certain',\n",
       " 'dump',\n",
       " 'made',\n",
       " 'period',\n",
       " 'should',\n",
       " 'saying',\n",
       " 'agreement',\n",
       " 'resources',\n",
       " 'kennel',\n",
       " 'backed',\n",
       " 'look',\n",
       " 'need',\n",
       " 'shit',\n",
       " 'Sterling',\n",
       " 'austerity',\n",
       " 'Of',\n",
       " 'Poland',\n",
       " 'interests',\n",
       " 'scenario',\n",
       " 'declared',\n",
       " 'rather',\n",
       " 'see',\n",
       " 'collapse',\n",
       " 'almost',\n",
       " 'Just',\n",
       " 'buy',\n",
       " 'lose',\n",
       " 'under',\n",
       " 'body',\n",
       " 'consequences',\n",
       " 'us',\n",
       " 'resulted',\n",
       " 'far',\n",
       " 'investors',\n",
       " 'co-operation',\n",
       " 'ships',\n",
       " 'least',\n",
       " 'Hungary',\n",
       " 'skittish',\n",
       " 'believe',\n",
       " 'conquer',\n",
       " 'We',\n",
       " 'such',\n",
       " 'problem',\n",
       " 'strong',\n",
       " 'sea',\n",
       " 'boost',\n",
       " 'pretty',\n",
       " 'block',\n",
       " 'effect',\n",
       " 'killed',\n",
       " 'politicians',\n",
       " 'free',\n",
       " 'dogshit',\n",
       " 'next',\n",
       " 'turns',\n",
       " 'whatever',\n",
       " 'tariff',\n",
       " 'wanted',\n",
       " 'process',\n",
       " 'happened',\n",
       " 'day',\n",
       " 'Romania',\n",
       " 'issues',\n",
       " ';',\n",
       " 'idea',\n",
       " 'level',\n",
       " 'comes',\n",
       " 'bigger',\n",
       " 'flourished',\n",
       " 'growth',\n",
       " 'combined',\n",
       " 'situation',\n",
       " 'today',\n",
       " 'theirs',\n",
       " 'break',\n",
       " 'A',\n",
       " 'Trade',\n",
       " 'Navy',\n",
       " 'financial',\n",
       " 'although',\n",
       " 'massive',\n",
       " 'created',\n",
       " 'gain',\n",
       " 'choice',\n",
       " 'opportunity',\n",
       " 'single',\n",
       " 'gambling',\n",
       " 'where',\n",
       " 'eventually',\n",
       " 'price',\n",
       " 'remember',\n",
       " '[',\n",
       " 'stock',\n",
       " 'earth',\n",
       " '1989',\n",
       " 'union',\n",
       " 'important',\n",
       " 'functionning',\n",
       " 'pay',\n",
       " 'elected',\n",
       " 'against',\n",
       " 'worse',\n",
       " 'human',\n",
       " 'debt',\n",
       " 'call',\n",
       " 'reforms',\n",
       " 'two',\n",
       " 'words',\n",
       " 'Which',\n",
       " 'regard',\n",
       " 'common',\n",
       " 'towards',\n",
       " 'presented',\n",
       " 'British',\n",
       " 'history',\n",
       " 'Euro',\n",
       " 'legislation',\n",
       " 'give',\n",
       " ']',\n",
       " 'dissolved',\n",
       " '15th',\n",
       " 'never']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[0] for x in sortedEntities if x[1] == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also list the most common \"non-objects\". (We note that we're not graphing these because there are so few here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('EU', 38),\n",
       " ('UK', 21),\n",
       " ('Britain', 17),\n",
       " ('Germany', 9),\n",
       " ('US', 7),\n",
       " ('Greece', 7),\n",
       " ('USSR', 5),\n",
       " ('Spain', 4),\n",
       " ('Europe', 4),\n",
       " ('Norway', 4)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortedNonObj = count_nonobjs(cmv_df['classified_sents'])\n",
    "sortedNonObj[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, 'EU', 'UK', and 'Britain' take the top three spots, but it might be interesting to take a deeper look at the contexts in which the other countries come up. We might think that what is being referenced is the state of their economies but we can use parsing to confirm this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the Organizations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('EU', 31),\n",
       " ('Navy', 2),\n",
       " ('US', 2),\n",
       " ('Union', 2),\n",
       " ('European', 2),\n",
       " ('Wikipedia', 1),\n",
       " ('Indian', 1),\n",
       " ('Trade', 1),\n",
       " ('West', 1),\n",
       " ('Pact', 1)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortedOrgs = count_orgs(cmv_df['classified_sents'])\n",
    "sortedOrgs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution suggests that the focus of my corpus is, unsurprisingly, the European Union and the United Kingdom. Oddly enough it did not seem to pick up 'EU' as referring to an organization all 38 times it was mentioned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at how accurately the organization 'EU' (The European Union) was detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmcclellan/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "def calc_EU_prf(classified_sents):\n",
    "    '''\n",
    "    '''\n",
    "    real_EU = [] # Should be all 1s since every instance of 'EU' refers to the European Union\n",
    "    pred_EU = [] # 1 for everytime 'EU' classified as an organization, 0 otherwise\n",
    "    \n",
    "    \n",
    "    for entry in classified_sents:\n",
    "        for sentence in entry:\n",
    "            for ent, kind in sentence:\n",
    "                if ent != 'EU':\n",
    "                    continue\n",
    "                else:\n",
    "                    real_EU.append(1)\n",
    "                    if kind == 'ORGANIZATION':\n",
    "                        pred_val = 1\n",
    "                    else:\n",
    "                        pred_val = 0\n",
    "                    pred_EU.append(pred_val)\n",
    "    \n",
    "#   return(real_EU, pred_EU)\n",
    "    prf = sklearn.metrics.precision_recall_fscore_support(np.array(real_EU), np.array(pred_EU))\n",
    "    return(prf)\n",
    "    \n",
    "EU_prf = calc_EU_prf(cmv_df['classified_sents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.]\n",
      "[ 0.          0.81578947]\n",
      "[ 0.          0.89855072]\n"
     ]
    }
   ],
   "source": [
    "precision, recall, fscore, _ = EU_prf\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We face an issue because in our 'true' array we don't actually have any negative values, so we encounter a bit of a problem trying to calculate recall and f-score, since recall has 'False negatives' in the denominator, which is undefined if there are no negatives in the first place.\n",
    "\n",
    "Since I don't want to manually comb the text to find which instances of EU are not actually referring to the European Union (Indeed, I would imagine all of the instances are referring to the European Union.), I will leave this as is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Your turn (Parsing)*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, parse a (modest) subset of your corpus of interest. How deep are the phrase structure and dependency parse trees nested? How does parse depth relate to perceived sentence complexity? What are five things you can extract from these parses for subsequent analysis? (e.g., nouns collocated in a noun phrase; adjectives that modify a noun; etc.) Capture these sets of things for a focal set of words (e.g., \"Bush\", \"Obama\", \"Trump\"). What do they reveal about the roles that these entities are perceive to play in the social world inscribed by your texts?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the longest reply, containing a decent number of sentences, to parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rand_com = cmv_df.iloc[0] \n",
    "# cmv_df is sorted in descending character length so we get the longest\n",
    "# comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'>today the pound has dropped 13% in value.\\n\\nThe pound was already far more powerful than the Euro and the Dollar. That\\'s why you guys never adopted the Euro in the first place. So point one is that, even if it drops 13% you\\'re still a powerful economy, and it\\'s not as though your country can\\'t recover. Which leads me to point number 2. Money trading is just like the stock market. Investors are skittish, because they want to make investing (which is gambling) not _feel like_ gambling. So the slightest scare in their emotional hearts gets them to pull out of otherwise totally fine investments, like the British pound. What\\'s worse is that the smart investors notice the skittish ones and pull out, too, but only so they can buy back the stuff at the much lower price. It happens in every stock or currency market all over the world. However in the long run it\\'s just a blip where the stuff changes hands quickly and the long-run value stays on its normal course, which for England, is up.\\n\\n>meaning we will likely still have to trade with the EU under EU regulations.\\n\\nSure, but you\\'ll get to trade with the US, Russia, and literally every other country on earth according to your own rules. Moreover, it\\'s not like the EU will penalize your country for leaving: France, the Netherlands, and Germany have all considered it, too, and they\\'re the backbone of the whole thing. It\\'s not like you just declared war on them, you just didn\\'t want to be roommates anymore. They still have a lot to lose from not trading with the UK, but now you can trade with other people without their say so in your affairs.\\n\\n>Cameron (who I\\'m not a fan of) has stepped down, meaning we could potentially end up with the second unelected prime minister in my lifetime.\\n\\nSo? Cameron shouldn\\'t have been there in the first place if he was going to throw a hissy fit for not getting his way. \\n\\nThe main point is this: the EU was presented as a fairly low-commitment treaty between European nations. You get to have a trading block to collectively bargain against countries like the US. You get a single currency so you\\'re not going to have to mess around with Deutschmarks or Franks or Kroner or whatever other bullshit your neighbors call money, and you get a single passport so you can go to the country next to you without a visa or border guards or whatever other bullshit used to be there. It was presented as a petty bullshit minimizer, and people all over Europe bought into it, and rightly so. But that\\'s not what it turned into. It gradually became a rhetorical \"out\" for entrenched politicians to excuse their laziness in taking care of internal national issues. Now there\\'s a balance of political power (because money is always tied to power, and big trade agreements require both) residing in the hands of people associated with the trade block, not the people. It\\'s become a lumbering bureaucracy that can\\'t be held accountable to any national body: in essence a bullshit contributor. If at any point the bullshit added to your country\\'s life exceeds the bullshit solved by being in the EU, then the rational choice is to leave.\\n\\nThis isn\\'t to say that it was ultimately a good choice: self-determination is a risky endeavor. But it is enough to say that rational people can hold the opinion that leaving the EU is not a terrible unadulterated evil.'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_com['com_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parses = list(parser.parse_sents(rand_com['sentences'])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the average depth of the phase structure trees created for this comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For one of the longer comments then, it seems that the average phase structure tree depth (with one tree for each sentence) is 12.6\n"
     ]
    }
   ],
   "source": [
    "tot_trees, cum_height = 0, 0\n",
    "got_example = False\n",
    "parses_for_relations = []\n",
    "\n",
    "for thing in parses: \n",
    "    thing = list(thing)\n",
    "    tot_trees += 1\n",
    "    tree = thing[0]\n",
    "    \n",
    "    parses_for_relations.append(tree)\n",
    "    cum_height += tree.height()\n",
    "    \n",
    "avg_height = cum_height / tot_trees\n",
    "print('For one of the longer comments then, it seems that the average phase structure tree depth (with one '\n",
    "      'tree for each sentence) is {}'.format(avg_height))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Five things for Subsequent Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that in order to obtain more useful results, I have created wrapper functions so that I can work over multiple trees (i.e. multiple sentences)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The average depth of a phase structure can be used as an approximation of average sentence complexity within a comment. (The depth of a phase structure is proportional to perceived sentence complexity.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we saw that for one of the longer comments, the average phase structure tree depth is 12.6. \n",
    "\n",
    "If another comment had an average phase structure depth of, let's say, 7, then we might reasonably conclude that that the comment we are looking at features more complex sentences on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Noun pharses in which the word 'trade' appears in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('NP',\n",
       "   'a balance of political power -LRB- because money is always tied to power , and big trade agreements require both -RRB- residing in the hands of people associated with the trade block , not the people')],\n",
       " [('NP',\n",
       "   'political power -LRB- because money is always tied to power , and big trade agreements require both -RRB- residing in the hands of people associated with the trade block , not the people')],\n",
       " [('NP', 'big trade agreements')],\n",
       " [('NP',\n",
       "   'the hands of people associated with the trade block , not the people')],\n",
       " [('NP', 'people associated with the trade block , not the people')],\n",
       " [('NP', 'the trade block , not the people')],\n",
       " [('NP', 'the trade block')]]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treesRelation(parses_for_relations, 'NP', 'trade')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is only for one comment, but we can see the power that trees have, we can see the specific contexts in which words appears in noun phrases. If only we could get it to run for more things. . . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The verb phrases in which the word 'economy' appears in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('VP',\n",
       "   \"is that , even if it drops 13 % you 're still a powerful economy , and it 's not as though your country ca n't recover\")],\n",
       " [('VP', \"'re still a powerful economy\")]]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treesRelation(parses_for_relations, 'VP', 'economy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The noun phrases/verb in which multiple words ('trade', 'EU') appear in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('VP', 'will likely still have to trade with the EU under EU regulations')],\n",
       " [('VP', 'have to trade with the EU under EU regulations')],\n",
       " [('VP', 'to trade with the EU under EU regulations')],\n",
       " [('VP', 'trade with the EU under EU regulations')]]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treesRelation(parses_for_relations, 'VP', 'trade', 'EU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is simply a subset of what we collected earlier, but the utility is obvious. We can obtain verb phrases and noun phrases with arbitrarily complex contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The adjectives that modify the word 'trade' within a noun phrase/verb phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'big', 'political'}"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treesSubRelation(parses_for_relations, 'NP', 'JJ', 'trade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'big', 'other', 'own', 'political'}"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treesSubRelation(parses_for_relations, 'VP', 'JJ', 'trade')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it is interesting enough in its own right to look at the adjectives that modify a noun within a nounphrase, semantic analysis could be conducted on these words, and their would be more to choose if you ran it on a larger corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at a tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                              ROOT                                                                                  \n",
      "                                                               |                                                                                     \n",
      "                                                               S                                                                                    \n",
      "  _____________________________________________________________|__________________________________________________________________________________   \n",
      " |   |        VP                                                                                                                                  | \n",
      " |   |    ____|_____                                                                                                                              |  \n",
      " |   |   |         ADJP                                                                                                                           | \n",
      " |   |   |     _____|____                                                                                                                         |  \n",
      " |   |   |    |          S                                                                                                                        | \n",
      " |   |   |    |          |                                                                                                                        |  \n",
      " |   |   |    |          VP                                                                                                                       | \n",
      " |   |   |    |      ____|__________                                                                                                              |  \n",
      " |   |   |    |     |               VP                                                                                                            | \n",
      " |   |   |    |     |     __________|_____________________                                                                                        |  \n",
      " |   |   |    |     |    |                               SBAR                                                                                     | \n",
      " |   |   |    |     |    |    ____________________________|____________________________                                                           |  \n",
      " |   |   |    |     |    |   |                                                         S                                                          | \n",
      " |   |   |    |     |    |   |              ___________________________________________|_____                                                     |  \n",
      " |   |   |    |     |    |   |             |                                                 VP                                                   | \n",
      " |   |   |    |     |    |   |             |           ______________________________________|_______                                             |  \n",
      " |   |   |    |     |    |   |             |          |                                              VP                                           | \n",
      " |   |   |    |     |    |   |             |          |    __________________________________________|___                                         |  \n",
      " |   |   |    |     |    |   |             |          |   |         |                                   SBAR                                      | \n",
      " |   |   |    |     |    |   |             |          |   |         |            ________________________|____                                    |  \n",
      " |   |   |    |     |    |   |             |          |   |         |           |                             S                                   | \n",
      " |   |   |    |     |    |   |             |          |   |         |           |             ________________|___                                |  \n",
      " |   |   |    |     |    |   |             |          |   |         |           |            S                    |                               | \n",
      " |   |   |    |     |    |   |             |          |   |         |           |            |                    |                               |  \n",
      " |   |   |    |     |    |   |             |          |   |         |           |            VP                   VP                              | \n",
      " |   |   |    |     |    |   |             |          |   |         |           |       _____|___         ________|_____                          |  \n",
      " |   NP  |    |     |    |   |             NP         |   |         NP          |      |         NP      |    |         NP                        | \n",
      " |   |   |    |     |    |   |       ______|____      |   |     ____|_____      |      |      ___|___    |    |    _____|____________________     |  \n",
      " CC PRP VBZ   JJ    TO   VB  IN     JJ         NNS    MD  VB   DT         NN    IN    VBG    DT      NN VBZ   RB  DT    JJ          JJ       NN   . \n",
      " |   |   |    |     |    |   |      |           |     |   |    |          |     |      |     |       |   |    |   |     |           |        |    |  \n",
      "But  it  is enough  to  say that rational     people can hold the      opinion that leaving the      EU  is  not  a  terrible unadulterated evil  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ex_tree[0].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Parse Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the examples, Dependency parse trees weren't further analyzed to get any useful information, but we'll make some and show one graphically anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "depParses = list(depParser.raw_parse_sents(rand_com['each_sentence_is_list'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tot_trees, cum_height = 0, 0\n",
    "got_example = False\n",
    "\n",
    "for thing in depParses: \n",
    "    thing = list(thing)\n",
    "    if not got_example:\n",
    "        ex_tree = thing\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependency Parse trees do not have a height attribute, so let's just look at tone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.36.0 (20140111.2315)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"504pt\" height=\"484pt\"\n",
       " viewBox=\"0.00 0.00 504.00 484.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 480)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-480 500,-480 500,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\n",
       "<text text-anchor=\"middle\" x=\"269\" y=\"-454.3\" font-family=\"Times,serif\" font-size=\"14.00\">0 (None)</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node2\" class=\"node\"><title>8</title>\n",
       "<text text-anchor=\"middle\" x=\"269\" y=\"-366.3\" font-family=\"Times,serif\" font-size=\"14.00\">8 (found)</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;8 -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M269,-439.597C269,-427.746 269,-411.817 269,-398.292\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"272.5,-398.084 269,-388.084 265.5,-398.084 272.5,-398.084\"/>\n",
       "<text text-anchor=\"middle\" x=\"280\" y=\"-410.3\" font-family=\"Times,serif\" font-size=\"14.00\">root</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\"><title>2</title>\n",
       "<text text-anchor=\"middle\" x=\"109\" y=\"-278.3\" font-family=\"Times,serif\" font-size=\"14.00\">2 (empire)</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;2 -->\n",
       "<g id=\"edge9\" class=\"edge\"><title>8&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M237.006,-351.803C211.853,-338.283 176.651,-319.363 149.528,-304.784\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"151.131,-301.672 140.665,-300.02 147.817,-307.838 151.131,-301.672\"/>\n",
       "<text text-anchor=\"middle\" x=\"219\" y=\"-322.3\" font-family=\"Times,serif\" font-size=\"14.00\">nsubj</text>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node10\" class=\"node\"><title>11</title>\n",
       "<text text-anchor=\"middle\" x=\"269\" y=\"-278.3\" font-family=\"Times,serif\" font-size=\"14.00\">11 (travelling)</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;11 -->\n",
       "<g id=\"edge8\" class=\"edge\"><title>8&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M269,-351.597C269,-339.746 269,-323.817 269,-310.292\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"272.5,-310.084 269,-300.084 265.5,-310.084 272.5,-310.084\"/>\n",
       "<text text-anchor=\"middle\" x=\"284\" y=\"-322.3\" font-family=\"Times,serif\" font-size=\"14.00\">advcl</text>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node11\" class=\"node\"><title>9</title>\n",
       "<text text-anchor=\"middle\" x=\"378\" y=\"-278.3\" font-family=\"Times,serif\" font-size=\"14.00\">9 (resources)</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;9 -->\n",
       "<g id=\"edge10\" class=\"edge\"><title>8&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M290.796,-351.803C307.282,-338.796 330.103,-320.79 348.265,-306.461\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"350.559,-309.109 356.242,-300.167 346.223,-303.613 350.559,-309.109\"/>\n",
       "<text text-anchor=\"middle\" x=\"346.5\" y=\"-322.3\" font-family=\"Times,serif\" font-size=\"14.00\">dobj</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\"><title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-190.3\" font-family=\"Times,serif\" font-size=\"14.00\">1 (An)</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;1 -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>2&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M92.4061,-263.597C80.3234,-250.925 63.7945,-233.589 50.3761,-219.516\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"52.7233,-216.906 43.2894,-212.084 47.6571,-221.737 52.7233,-216.906\"/>\n",
       "<text text-anchor=\"middle\" x=\"84.5\" y=\"-234.3\" font-family=\"Times,serif\" font-size=\"14.00\">det</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\"><title>4</title>\n",
       "<text text-anchor=\"middle\" x=\"109\" y=\"-190.3\" font-family=\"Times,serif\" font-size=\"14.00\">4 (Britain)</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;4 -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>2&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M109,-263.597C109,-251.746 109,-235.817 109,-222.292\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"112.5,-222.084 109,-212.084 105.5,-222.084 112.5,-222.084\"/>\n",
       "<text text-anchor=\"middle\" x=\"125\" y=\"-234.3\" font-family=\"Times,serif\" font-size=\"14.00\">nmod</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node6\" class=\"node\"><title>3</title>\n",
       "<text text-anchor=\"middle\" x=\"41\" y=\"-102.3\" font-family=\"Times,serif\" font-size=\"14.00\">3 (like)</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;3 -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>4&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M95.2392,-175.597C85.4049,-163.159 72.0189,-146.23 61.0058,-132.302\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.4561,-129.757 54.5083,-124.084 57.9652,-134.099 63.4561,-129.757\"/>\n",
       "<text text-anchor=\"middle\" x=\"94\" y=\"-146.3\" font-family=\"Times,serif\" font-size=\"14.00\">case</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node7\" class=\"node\"><title>7</title>\n",
       "<text text-anchor=\"middle\" x=\"116\" y=\"-102.3\" font-family=\"Times,serif\" font-size=\"14.00\">7 (past)</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;7 -->\n",
       "<g id=\"edge5\" class=\"edge\"><title>4&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M110.417,-175.597C111.381,-163.746 112.678,-147.817 113.779,-134.292\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"117.287,-134.335 114.609,-124.084 110.31,-133.767 117.287,-134.335\"/>\n",
       "<text text-anchor=\"middle\" x=\"130\" y=\"-146.3\" font-family=\"Times,serif\" font-size=\"14.00\">nmod</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node8\" class=\"node\"><title>5</title>\n",
       "<text text-anchor=\"middle\" x=\"44\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">5 (in)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;5 -->\n",
       "<g id=\"edge6\" class=\"edge\"><title>7&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M101.43,-87.5966C90.9188,-75.0419 76.5758,-57.9099 64.8533,-43.9081\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"67.406,-41.5048 58.3029,-36.084 62.0386,-45.9984 67.406,-41.5048\"/>\n",
       "<text text-anchor=\"middle\" x=\"99\" y=\"-58.3\" font-family=\"Times,serif\" font-size=\"14.00\">case</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node9\" class=\"node\"><title>6</title>\n",
       "<text text-anchor=\"middle\" x=\"116\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">6 (the)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;6 -->\n",
       "<g id=\"edge7\" class=\"edge\"><title>7&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M116,-87.5966C116,-75.7459 116,-59.8169 116,-46.2917\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"119.5,-46.084 116,-36.084 112.5,-46.084 119.5,-46.084\"/>\n",
       "<text text-anchor=\"middle\" x=\"124.5\" y=\"-58.3\" font-family=\"Times,serif\" font-size=\"14.00\">det</text>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node12\" class=\"node\"><title>10</title>\n",
       "<text text-anchor=\"middle\" x=\"192\" y=\"-190.3\" font-family=\"Times,serif\" font-size=\"14.00\">10 (by)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;10 -->\n",
       "<g id=\"edge13\" class=\"edge\"><title>11&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M252.397,-263.694C247.104,-258.097 241.258,-251.829 236,-246 228.403,-237.577 220.245,-228.215 213.044,-219.835\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"215.641,-217.487 206.481,-212.164 210.322,-222.038 215.641,-217.487\"/>\n",
       "<text text-anchor=\"middle\" x=\"250.5\" y=\"-234.3\" font-family=\"Times,serif\" font-size=\"14.00\">mark</text>\n",
       "</g>\n",
       "<!-- 15 -->\n",
       "<g id=\"node13\" class=\"node\"><title>15</title>\n",
       "<text text-anchor=\"middle\" x=\"269\" y=\"-190.3\" font-family=\"Times,serif\" font-size=\"14.00\">15 (and)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;15 -->\n",
       "<g id=\"edge11\" class=\"edge\"><title>11&#45;&gt;15</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M269,-263.597C269,-251.746 269,-235.817 269,-222.292\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"272.5,-222.084 269,-212.084 265.5,-222.084 272.5,-222.084\"/>\n",
       "<text text-anchor=\"middle\" x=\"275.5\" y=\"-234.3\" font-family=\"Times,serif\" font-size=\"14.00\">cc</text>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node14\" class=\"node\"><title>16</title>\n",
       "<text text-anchor=\"middle\" x=\"361\" y=\"-190.3\" font-family=\"Times,serif\" font-size=\"14.00\">16 (stealing)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;16 -->\n",
       "<g id=\"edge12\" class=\"edge\"><title>11&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M287.618,-263.597C301.299,-250.807 320.062,-233.268 335.19,-219.126\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"337.809,-221.47 342.724,-212.084 333.029,-216.356 337.809,-221.47\"/>\n",
       "<text text-anchor=\"middle\" x=\"336\" y=\"-234.3\" font-family=\"Times,serif\" font-size=\"14.00\">conj</text>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node15\" class=\"node\"><title>13</title>\n",
       "<text text-anchor=\"middle\" x=\"459\" y=\"-190.3\" font-family=\"Times,serif\" font-size=\"14.00\">13 (world)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;13 -->\n",
       "<g id=\"edge14\" class=\"edge\"><title>11&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M310.535,-263.923C323.836,-258.338 338.589,-252.02 352,-246 372.813,-236.658 395.665,-225.843 414.888,-216.582\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"416.527,-219.677 424.008,-212.175 413.481,-213.375 416.527,-219.677\"/>\n",
       "<text text-anchor=\"middle\" x=\"397.5\" y=\"-234.3\" font-family=\"Times,serif\" font-size=\"14.00\">dobj</text>\n",
       "</g>\n",
       "<!-- 20 -->\n",
       "<g id=\"node17\" class=\"node\"><title>20</title>\n",
       "<text text-anchor=\"middle\" x=\"279\" y=\"-102.3\" font-family=\"Times,serif\" font-size=\"14.00\">20 (cultures)</text>\n",
       "</g>\n",
       "<!-- 16&#45;&gt;20 -->\n",
       "<g id=\"edge16\" class=\"edge\"><title>16&#45;&gt;20</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M344.406,-175.597C332.323,-162.925 315.794,-145.589 302.376,-131.516\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"304.723,-128.906 295.289,-124.084 299.657,-133.737 304.723,-128.906\"/>\n",
       "<text text-anchor=\"middle\" x=\"344\" y=\"-146.3\" font-family=\"Times,serif\" font-size=\"14.00\">nmod</text>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node18\" class=\"node\"><title>17</title>\n",
       "<text text-anchor=\"middle\" x=\"376\" y=\"-102.3\" font-family=\"Times,serif\" font-size=\"14.00\">17 (them)</text>\n",
       "</g>\n",
       "<!-- 16&#45;&gt;17 -->\n",
       "<g id=\"edge17\" class=\"edge\"><title>16&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M364.035,-175.597C366.102,-163.746 368.881,-147.817 371.24,-134.292\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"374.75,-134.537 373.02,-124.084 367.854,-133.334 374.75,-134.537\"/>\n",
       "<text text-anchor=\"middle\" x=\"382.5\" y=\"-146.3\" font-family=\"Times,serif\" font-size=\"14.00\">dobj</text>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node16\" class=\"node\"><title>12</title>\n",
       "<text text-anchor=\"middle\" x=\"459\" y=\"-102.3\" font-family=\"Times,serif\" font-size=\"14.00\">12 (the)</text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;12 -->\n",
       "<g id=\"edge15\" class=\"edge\"><title>13&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M459,-175.597C459,-163.746 459,-147.817 459,-134.292\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"462.5,-134.084 459,-124.084 455.5,-134.084 462.5,-134.084\"/>\n",
       "<text text-anchor=\"middle\" x=\"467.5\" y=\"-146.3\" font-family=\"Times,serif\" font-size=\"14.00\">det</text>\n",
       "</g>\n",
       "<!-- 18 -->\n",
       "<g id=\"node19\" class=\"node\"><title>18</title>\n",
       "<text text-anchor=\"middle\" x=\"203\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">18 (from)</text>\n",
       "</g>\n",
       "<!-- 20&#45;&gt;18 -->\n",
       "<g id=\"edge19\" class=\"edge\"><title>20&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M263.62,-87.5966C252.525,-75.0419 237.386,-57.9099 225.012,-43.9081\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"227.342,-41.2596 218.097,-36.084 222.097,-45.895 227.342,-41.2596\"/>\n",
       "<text text-anchor=\"middle\" x=\"260\" y=\"-58.3\" font-family=\"Times,serif\" font-size=\"14.00\">case</text>\n",
       "</g>\n",
       "<!-- 19 -->\n",
       "<g id=\"node20\" class=\"node\"><title>19</title>\n",
       "<text text-anchor=\"middle\" x=\"292\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">19 (other)</text>\n",
       "</g>\n",
       "<!-- 20&#45;&gt;19 -->\n",
       "<g id=\"edge18\" class=\"edge\"><title>20&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M281.631,-87.5966C283.422,-75.7459 285.83,-59.8169 287.875,-46.2917\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"291.383,-46.4948 289.418,-36.084 284.462,-45.4485 291.383,-46.4948\"/>\n",
       "<text text-anchor=\"middle\" x=\"302.5\" y=\"-58.3\" font-family=\"Times,serif\" font-size=\"14.00\">amod</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<DependencyGraph with 20 nodes>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_tree[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information extraction (INSTRUCTOR)\n",
    "\n",
    "Information extraction approaches typically (as here, with Stanford's Open IE engine) ride atop the dependency parse of a sentence. They are a pre-coded example of the type analyzed in the prior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting OpenIE run\n",
      "Adding annotator tokenize\n",
      "Adding annotator ssplit\n",
      "Adding annotator pos\n",
      "Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.9 sec].\n",
      "Adding annotator lemma\n",
      "Adding annotator depparse\n",
      "Loading depparse model file: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... \n",
      "PreComputed 99996, Elapsed Time: 16.684 (s)\n",
      "Initializing dependency parser ... done [18.4 sec].\n",
      "Adding annotator natlog\n",
      "Adding annotator openie\n",
      "Loading clause splitter from edu/stanford/nlp/models/naturalli/clauseSearcherModel.ser.gz ... done [0.0149 seconds]\n",
      "Processing file: /tmp/tmp98exllr1\n",
      "All files have been queued; awaiting termination...\n",
      "DONE processing files. 0 exceptions encountered.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ieDF = openIE(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`openIE()` prints everything stanford core produces and we can see from looking at it that initializing the dependency parser takes most of the time, so calling the function will always take at least 12 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>certainty</th>\n",
       "      <th>subject</th>\n",
       "      <th>verb</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>elephant</td>\n",
       "      <td>is in</td>\n",
       "      <td>my pajamas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I</td>\n",
       "      <td>saw</td>\n",
       "      <td>elephant in my pajamas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I</td>\n",
       "      <td>saw</td>\n",
       "      <td>elephant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>quick brown fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>lazy dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>quick brown fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>quick fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>brown fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>lazy dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>brown fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>quick fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>lazy dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>lazy dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>short-term stimulus efforts in interview with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>stimulus efforts in interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>stimulus efforts in recent interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed stimulus efforts in</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>short-term stimulus efforts in recent intervie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>stimulus efforts in recent interview with Wall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>short-term stimulus efforts in recent interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>short-term stimulus efforts</td>\n",
       "      <td>is in</td>\n",
       "      <td>recent interview with Wall Street Journal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>stimulus efforts in interview with Wall Street...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>stimulus efforts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>short-term stimulus efforts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>short-term stimulus efforts in interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>recent interview</td>\n",
       "      <td>is with</td>\n",
       "      <td>Wall Street Journal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Martin</td>\n",
       "      <td>was</td>\n",
       "      <td>African</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Trayvon Benjamin Martin</td>\n",
       "      <td>was African American from</td>\n",
       "      <td>Florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Trayvon Benjamin Martin</td>\n",
       "      <td>was American from</td>\n",
       "      <td>Florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Trayvon Benjamin Martin</td>\n",
       "      <td>was</td>\n",
       "      <td>American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Trayvon Benjamin Martin</td>\n",
       "      <td>was</td>\n",
       "      <td>African American</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    certainty                      subject                           verb  \\\n",
       "0         1.0                     elephant                          is in   \n",
       "1         1.0                            I                            saw   \n",
       "2         1.0                            I                            saw   \n",
       "3         1.0              quick brown fox                    jumped over   \n",
       "4         1.0              quick brown fox                    jumped over   \n",
       "5         1.0                    quick fox                    jumped over   \n",
       "6         1.0                          fox                    jumped over   \n",
       "7         1.0                    brown fox                    jumped over   \n",
       "8         1.0                    brown fox                    jumped over   \n",
       "9         1.0                    quick fox                    jumped over   \n",
       "10        1.0                          fox                    jumped over   \n",
       "11        1.0            Christine Lagarde                      discussed   \n",
       "12        1.0            Christine Lagarde                      discussed   \n",
       "13        1.0            Christine Lagarde                      discussed   \n",
       "14        1.0            Christine Lagarde  discussed stimulus efforts in   \n",
       "15        1.0            Christine Lagarde                      discussed   \n",
       "16        1.0            Christine Lagarde                      discussed   \n",
       "17        1.0            Christine Lagarde                      discussed   \n",
       "18        1.0  short-term stimulus efforts                          is in   \n",
       "19        1.0            Christine Lagarde                      discussed   \n",
       "20        1.0            Christine Lagarde                      discussed   \n",
       "21        1.0            Christine Lagarde                      discussed   \n",
       "22        1.0            Christine Lagarde                      discussed   \n",
       "23        1.0             recent interview                        is with   \n",
       "24        1.0                       Martin                            was   \n",
       "25        1.0      Trayvon Benjamin Martin      was African American from   \n",
       "26        1.0      Trayvon Benjamin Martin              was American from   \n",
       "27        1.0      Trayvon Benjamin Martin                            was   \n",
       "28        1.0      Trayvon Benjamin Martin                            was   \n",
       "\n",
       "                                               object  \n",
       "0                                          my pajamas  \n",
       "1                              elephant in my pajamas  \n",
       "2                                            elephant  \n",
       "3                                            lazy dog  \n",
       "4                                                 dog  \n",
       "5                                                 dog  \n",
       "6                                                 dog  \n",
       "7                                            lazy dog  \n",
       "8                                                 dog  \n",
       "9                                            lazy dog  \n",
       "10                                           lazy dog  \n",
       "11  short-term stimulus efforts in interview with ...  \n",
       "12                      stimulus efforts in interview  \n",
       "13               stimulus efforts in recent interview  \n",
       "14                                             France  \n",
       "15  short-term stimulus efforts in recent intervie...  \n",
       "16  stimulus efforts in recent interview with Wall...  \n",
       "17    short-term stimulus efforts in recent interview  \n",
       "18          recent interview with Wall Street Journal  \n",
       "19  stimulus efforts in interview with Wall Street...  \n",
       "20                                   stimulus efforts  \n",
       "21                        short-term stimulus efforts  \n",
       "22           short-term stimulus efforts in interview  \n",
       "23                                Wall Street Journal  \n",
       "24                                            African  \n",
       "25                                            Florida  \n",
       "26                                            Florida  \n",
       "27                                           American  \n",
       "28                                   African American  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No buffalos (because there were no verbs), but the rest is somewhat promising. Note, however, that it abandoned the key theme of the sentence about the tragic Trayvon Martin death (\"fatally shot\"), likely because it was buried so deeply within the complex phrase structure. This is obviously a challenge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Your thoughts (Interpret Instructor IE)*</span>\n",
    "\n",
    "<span style=\"color:red\">How would you extract relevant information about the Trayvon Martin sentence directly from the dependency parse (above)? Code an example here. (For instance, what compound nouns show up with what verb phrases within the sentence?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of the Trayvon Martin sentence, in order to discover what compound nouns show up with what verb phrases within the sentence, I would first discover all of the unique compound nouns within that sentence, and then see which verb phrases occur with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['African American', 'Trayvon Benjamin Martin'], dtype=object)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract only the Trayvon Martin sentence data\n",
    "ieDF = ieDF.loc[24:]\n",
    "\n",
    "# Discover all the compound nouns\n",
    "# Note that np.unique is not necessary in this case, but it would be \n",
    "# useful if a compound noun showed up as a verb and an object\n",
    "comp_nouns = np.unique(np.union1d(ieDF[ieDF['subject'].str.contains(' ')]['subject'], ieDF[ieDF['object'].str.contains(' ')]['object']))\n",
    "comp_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'African American': ['was'],\n",
       " 'Trayvon Benjamin Martin': ['was',\n",
       "  'was African American from',\n",
       "  'was American from']}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn_dict = {cn: None for cn in comp_nouns}\n",
    "for cn in comp_nouns:\n",
    "    verbs = list(np.unique(ieDF[(ieDF['subject'] == cn) | (ieDF['object'] == cn)]['verb']))\n",
    "    cn_dict[cn] = verbs\n",
    "\n",
    "\n",
    "# Here is the dictionary where the keys are the compound nouns and the values are lists of verbs\n",
    "# that appear with those compound nouns.\n",
    "cn_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <span style=\"color:red\">*Your Turn (IE on Brexit Replies)*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, perform open information extraction on a modest subset of texts relevant to your final project. Analyze the relative attachment of several subjects relative to verbs and objects and visa versa? Describe how you would select among these statements to create a database of high-value statements for your project and do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run information extraction on 5 replies to the Brexit submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting OpenIE run\n",
      "Adding annotator tokenize\n",
      "Adding annotator ssplit\n",
      "Adding annotator pos\n",
      "Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.1 sec].\n",
      "Adding annotator lemma\n",
      "Adding annotator depparse\n",
      "Loading depparse model file: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... \n",
      "PreComputed 99996, Elapsed Time: 17.317 (s)\n",
      "Initializing dependency parser ... done [19.1 sec].\n",
      "Adding annotator natlog\n",
      "Adding annotator openie\n",
      "Loading clause splitter from edu/stanford/nlp/models/naturalli/clauseSearcherModel.ser.gz ... done [0.0155 seconds]\n",
      "Processing file: /tmp/tmpzn30nm09\n",
      "All files have been queued; awaiting termination...\n",
      "DONE processing files. 0 exceptions encountered.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cmv_ieDF = openIE(cmv_df.sample(n = 5)['each_sentence_is_list'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>certainty</th>\n",
       "      <th>subject</th>\n",
       "      <th>verb</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>It</td>\n",
       "      <td>has</td>\n",
       "      <td>important</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.217987</td>\n",
       "      <td>i</td>\n",
       "      <td>spoke</td>\n",
       "      <td>who voted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>trade</td>\n",
       "      <td>flourished in</td>\n",
       "      <td>sea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Britain</td>\n",
       "      <td>was land</td>\n",
       "      <td>trade commerce flourished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Britain</td>\n",
       "      <td>was</td>\n",
       "      <td>land</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Britain</td>\n",
       "      <td>was land</td>\n",
       "      <td>trade commerce flourished in Mediterranean sea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Britain</td>\n",
       "      <td>was land</td>\n",
       "      <td>trade flourished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Britain</td>\n",
       "      <td>Prior was land</td>\n",
       "      <td>commerce flourished in Mediterranean sea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Britain</td>\n",
       "      <td>was land</td>\n",
       "      <td>commerce flourished in Mediterranean sea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Britain</td>\n",
       "      <td>was land</td>\n",
       "      <td>trade commerce flourished in sea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Britain</td>\n",
       "      <td>was land</td>\n",
       "      <td>trade flourished in Mediterranean sea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Britain</td>\n",
       "      <td>Prior was land</td>\n",
       "      <td>trade commerce flourished in sea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>trade</td>\n",
       "      <td>flourished in</td>\n",
       "      <td>Mediterranean sea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Britain</td>\n",
       "      <td>was</td>\n",
       "      <td>Prior to century land of dog shit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Britain</td>\n",
       "      <td>Prior was land</td>\n",
       "      <td>commerce flourished in sea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>commerce</td>\n",
       "      <td>flourished in</td>\n",
       "      <td>Mediterranean sea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>commerce</td>\n",
       "      <td>flourished in</td>\n",
       "      <td>sea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Britain</td>\n",
       "      <td>was</td>\n",
       "      <td>Prior to 15th century land of dog shit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Britain</td>\n",
       "      <td>Prior was land</td>\n",
       "      <td>trade commerce flourished in Mediterranean sea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Britain</td>\n",
       "      <td>was land of</td>\n",
       "      <td>dog shit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Britain</td>\n",
       "      <td>Prior was land</td>\n",
       "      <td>trade flourished in Mediterranean sea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Britain</td>\n",
       "      <td>Prior was land</td>\n",
       "      <td>commerce flourished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Britain</td>\n",
       "      <td>was land</td>\n",
       "      <td>commerce flourished in sea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Britain</td>\n",
       "      <td>was land</td>\n",
       "      <td>trade flourished in sea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Britain</td>\n",
       "      <td>Prior was land</td>\n",
       "      <td>trade commerce flourished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Britain</td>\n",
       "      <td>Prior was land of</td>\n",
       "      <td>dog shit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Britain</td>\n",
       "      <td>was</td>\n",
       "      <td>Prior land</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Britain</td>\n",
       "      <td>was</td>\n",
       "      <td>Prior to 15th century land</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Britain</td>\n",
       "      <td>was</td>\n",
       "      <td>Prior to century land</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Britain</td>\n",
       "      <td>was land</td>\n",
       "      <td>commerce flourished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>empire</td>\n",
       "      <td>travelling</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>empire</td>\n",
       "      <td>resources in</td>\n",
       "      <td>past</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>empire</td>\n",
       "      <td>found resources in</td>\n",
       "      <td>past</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>empire</td>\n",
       "      <td>stealing</td>\n",
       "      <td>them</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>you</td>\n",
       "      <td>eventually run</td>\n",
       "      <td>steal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>you</td>\n",
       "      <td>run out of</td>\n",
       "      <td>places conquer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>empire</td>\n",
       "      <td>implodes on</td>\n",
       "      <td>itself</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>you</td>\n",
       "      <td>eventually run out of</td>\n",
       "      <td>places conquer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>you</td>\n",
       "      <td>run</td>\n",
       "      <td>steal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>It</td>\n",
       "      <td>cant</td>\n",
       "      <td>afford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>It</td>\n",
       "      <td>afford</td>\n",
       "      <td>exist without influx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>It</td>\n",
       "      <td>afford</td>\n",
       "      <td>exist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>It</td>\n",
       "      <td>afford</td>\n",
       "      <td>exist without influx of resources</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Britain</td>\n",
       "      <td>ran</td>\n",
       "      <td>conquer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Britain</td>\n",
       "      <td>ran out of</td>\n",
       "      <td>countries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>'s in</td>\n",
       "      <td>their interests</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Leave</td>\n",
       "      <td>wants</td>\n",
       "      <td>you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Both</td>\n",
       "      <td>are</td>\n",
       "      <td>wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>truth</td>\n",
       "      <td>is</td>\n",
       "      <td>actually mundane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>truth</td>\n",
       "      <td>is</td>\n",
       "      <td>mundane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>truth</td>\n",
       "      <td>is</td>\n",
       "      <td>actually rather mundane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>truth</td>\n",
       "      <td>is</td>\n",
       "      <td>rather mundane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>neo-liberalism</td>\n",
       "      <td>will remain</td>\n",
       "      <td>dominant ideology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>neo-liberalism</td>\n",
       "      <td>will remain</td>\n",
       "      <td>dominant economic ideology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>neo-liberalism</td>\n",
       "      <td>will remain</td>\n",
       "      <td>ideology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>neo-liberalism</td>\n",
       "      <td>will remain</td>\n",
       "      <td>economic ideology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Trade</td>\n",
       "      <td>only achieve</td>\n",
       "      <td>something</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Trade</td>\n",
       "      <td>achieve as</td>\n",
       "      <td>something</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Trade</td>\n",
       "      <td>only achieve as</td>\n",
       "      <td>something</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Trade</td>\n",
       "      <td>achieve</td>\n",
       "      <td>something</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    certainty         subject                   verb  \\\n",
       "0    1.000000              It                    has   \n",
       "1    0.217987               i                  spoke   \n",
       "2    1.000000           trade          flourished in   \n",
       "3    1.000000         Britain               was land   \n",
       "4    1.000000         Britain                    was   \n",
       "5    1.000000         Britain               was land   \n",
       "6    1.000000         Britain               was land   \n",
       "7    1.000000         Britain         Prior was land   \n",
       "8    1.000000         Britain               was land   \n",
       "9    1.000000         Britain               was land   \n",
       "10   1.000000         Britain               was land   \n",
       "11   1.000000         Britain         Prior was land   \n",
       "12   1.000000           trade          flourished in   \n",
       "13   1.000000         Britain                    was   \n",
       "14   1.000000         Britain         Prior was land   \n",
       "15   1.000000        commerce          flourished in   \n",
       "16   1.000000        commerce          flourished in   \n",
       "17   1.000000         Britain                    was   \n",
       "18   1.000000         Britain         Prior was land   \n",
       "19   1.000000         Britain            was land of   \n",
       "20   1.000000         Britain         Prior was land   \n",
       "21   1.000000         Britain         Prior was land   \n",
       "22   1.000000         Britain               was land   \n",
       "23   1.000000         Britain               was land   \n",
       "24   1.000000         Britain         Prior was land   \n",
       "25   1.000000         Britain      Prior was land of   \n",
       "26   1.000000         Britain                    was   \n",
       "27   1.000000         Britain                    was   \n",
       "28   1.000000         Britain                    was   \n",
       "29   1.000000         Britain               was land   \n",
       "..        ...             ...                    ...   \n",
       "70   1.000000          empire             travelling   \n",
       "71   1.000000          empire           resources in   \n",
       "72   1.000000          empire     found resources in   \n",
       "73   1.000000          empire               stealing   \n",
       "74   1.000000             you         eventually run   \n",
       "75   1.000000             you             run out of   \n",
       "76   1.000000          empire            implodes on   \n",
       "77   1.000000             you  eventually run out of   \n",
       "78   1.000000             you                    run   \n",
       "79   1.000000              It                   cant   \n",
       "80   1.000000              It                 afford   \n",
       "81   1.000000              It                 afford   \n",
       "82   1.000000              It                 afford   \n",
       "83   1.000000         Britain                    ran   \n",
       "84   1.000000         Britain             ran out of   \n",
       "85   1.000000              it                  's in   \n",
       "86   1.000000           Leave                  wants   \n",
       "87   1.000000            Both                    are   \n",
       "88   1.000000           truth                     is   \n",
       "89   1.000000           truth                     is   \n",
       "90   1.000000           truth                     is   \n",
       "91   1.000000           truth                     is   \n",
       "92   1.000000  neo-liberalism            will remain   \n",
       "93   1.000000  neo-liberalism            will remain   \n",
       "94   1.000000  neo-liberalism            will remain   \n",
       "95   1.000000  neo-liberalism            will remain   \n",
       "96   1.000000           Trade           only achieve   \n",
       "97   1.000000           Trade             achieve as   \n",
       "98   1.000000           Trade        only achieve as   \n",
       "99   1.000000           Trade                achieve   \n",
       "\n",
       "                                            object  \n",
       "0                                        important  \n",
       "1                                        who voted  \n",
       "2                                              sea  \n",
       "3                        trade commerce flourished  \n",
       "4                                             land  \n",
       "5   trade commerce flourished in Mediterranean sea  \n",
       "6                                 trade flourished  \n",
       "7         commerce flourished in Mediterranean sea  \n",
       "8         commerce flourished in Mediterranean sea  \n",
       "9                 trade commerce flourished in sea  \n",
       "10           trade flourished in Mediterranean sea  \n",
       "11                trade commerce flourished in sea  \n",
       "12                               Mediterranean sea  \n",
       "13               Prior to century land of dog shit  \n",
       "14                      commerce flourished in sea  \n",
       "15                               Mediterranean sea  \n",
       "16                                             sea  \n",
       "17          Prior to 15th century land of dog shit  \n",
       "18  trade commerce flourished in Mediterranean sea  \n",
       "19                                        dog shit  \n",
       "20           trade flourished in Mediterranean sea  \n",
       "21                             commerce flourished  \n",
       "22                      commerce flourished in sea  \n",
       "23                         trade flourished in sea  \n",
       "24                       trade commerce flourished  \n",
       "25                                        dog shit  \n",
       "26                                      Prior land  \n",
       "27                      Prior to 15th century land  \n",
       "28                           Prior to century land  \n",
       "29                             commerce flourished  \n",
       "..                                             ...  \n",
       "70                                           world  \n",
       "71                                            past  \n",
       "72                                            past  \n",
       "73                                            them  \n",
       "74                                           steal  \n",
       "75                                  places conquer  \n",
       "76                                          itself  \n",
       "77                                  places conquer  \n",
       "78                                           steal  \n",
       "79                                          afford  \n",
       "80                            exist without influx  \n",
       "81                                           exist  \n",
       "82               exist without influx of resources  \n",
       "83                                         conquer  \n",
       "84                                       countries  \n",
       "85                                 their interests  \n",
       "86                                             you  \n",
       "87                                           wrong  \n",
       "88                                actually mundane  \n",
       "89                                         mundane  \n",
       "90                         actually rather mundane  \n",
       "91                                  rather mundane  \n",
       "92                               dominant ideology  \n",
       "93                      dominant economic ideology  \n",
       "94                                        ideology  \n",
       "95                               economic ideology  \n",
       "96                                       something  \n",
       "97                                       something  \n",
       "98                                       something  \n",
       "99                                       something  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmv_ieDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the most common subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Britain                           35\n",
       "It                                 8\n",
       "empire                             5\n",
       "Trade                              4\n",
       "you                                4\n",
       "I                                  4\n",
       "Trades                             4\n",
       "neo-liberalism                     4\n",
       "truth                              4\n",
       "we                                 3\n",
       "world                              3\n",
       "us                                 3\n",
       "it                                 2\n",
       "trade                              2\n",
       "something countries                2\n",
       "process                            2\n",
       "commerce                           2\n",
       "ones                               2\n",
       "i                                  1\n",
       "We                                 1\n",
       "Britains advantageous position     1\n",
       "Leave                              1\n",
       "Both                               1\n",
       "Countries                          1\n",
       "country                            1\n",
       "Name: subject, dtype: int64"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmv_ieDF['subject'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the most common objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "something                                         4\n",
       "times                                             4\n",
       "trade flourished in Mediterranean sea             2\n",
       "past                                              2\n",
       "time                                              2\n",
       "steal                                             2\n",
       "sea                                               2\n",
       "dog shit                                          2\n",
       "commerce flourished in sea                        2\n",
       "us                                                2\n",
       "trade commerce flourished in sea                  2\n",
       "places conquer                                    2\n",
       "trade commerce flourished in Mediterranean sea    2\n",
       "trade flourished                                  2\n",
       "major                                             2\n",
       "commerce flourished                               2\n",
       "Mediterranean sea                                 2\n",
       "people                                            2\n",
       "commerce flourished in Mediterranean sea          2\n",
       "world                                             2\n",
       "trade commerce flourished                         2\n",
       "think                                             2\n",
       "trade flourished in sea                           2\n",
       "India                                             1\n",
       "you                                               1\n",
       "wrong                                             1\n",
       "countries                                         1\n",
       "dominant ideology                                 1\n",
       "world map                                         1\n",
       "big                                               1\n",
       "                                                 ..\n",
       "big navy                                          1\n",
       "course                                            1\n",
       "economic ideology                                 1\n",
       "notch                                             1\n",
       "actually rather mundane                           1\n",
       "problems                                          1\n",
       "two                                               1\n",
       "important                                         1\n",
       "stronger                                          1\n",
       "problems with                                     1\n",
       "only power                                        1\n",
       "rather mundane                                    1\n",
       "Singapore                                         1\n",
       "actually mundane                                  1\n",
       "Prior to 15th century land                        1\n",
       "ideology                                          1\n",
       "major power                                       1\n",
       "things might suck for while                       1\n",
       "mundane                                           1\n",
       "mistake                                           1\n",
       "pretty fucking big                                1\n",
       "power                                             1\n",
       "only major power                                  1\n",
       "itself                                            1\n",
       "land                                              1\n",
       "conquer                                           1\n",
       "exist                                             1\n",
       "Prior to century land of dog shit                 1\n",
       "back stronger                                     1\n",
       "who voted                                         1\n",
       "Name: object, dtype: int64"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmv_ieDF['object'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the verbs that are most commonly associated with 'you'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "run                      1\n",
       "run out of               1\n",
       "eventually run           1\n",
       "eventually run out of    1\n",
       "Name: verb, dtype: int64"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmv_ieDF[(cmv_ieDF['subject'] == 'you') | (cmv_ieDF['subject'] == 'You')]['verb'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the verbs that are most commonly associated with 'EU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "will likely negotiate    4\n",
       "will negotiate           4\n",
       "Name: verb, dtype: int64"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmv_ieDF[cmv_ieDF['subject'] == 'EU']['verb'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to extract high value statements from this information extraction. A general approach I would take is to find the most comment subjects and objects that are not pronouns, and take a look at the verbs associated with them. For example, above we see that EU is one of the most used subjects, and the associated verb pharses are 'will negotiate' and 'will likely negotiate'.]\n",
    "\n",
    "However, there does seem to be some difficulty associated with parsing more and more sentences. Indeed, when I attempted to run information extraction on all of the replies to the Brexit submission, even on my own computer, with 16GB ram, Java ran into memory issues. Information extraction, in the context of my project seems like it would on In order to efficiently utilize information extraction over a large volume of text, I would probably want to utilize some form of cluster computing. In this case, some sort of MPI implementation (e.g. mpi4py) would be ideal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memo \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this homework assignment, we looked at POS tagging with conditional associations,named entity recognition, parsing, and information extraction. In order to focus on a modest subuset of our corpora (/r/changemyview submissions and first tier comments), I decided to limit the subset of inquiry to comments from a single submission conerning the Brexit vote. \n",
    "\n",
    "    In POS tagging, we found that simple things like adjective and noun counts didn't hold a ton of immediately useful information, though their contents can be scanned to confirm some easy assumptions (i.e. our text in this case was on the topic we had explicitly chosen beforehand). Conditional associations were a little more interesting, providing us with contexts of words of interest.\n",
    "\n",
    "    Named entity recognition helped us to establish the 'players' of the social game (within the context of the analyzed corpora). In this case, the EU and the UK held top spot, with a few other countires referred to.\n",
    "\n",
    "    Parsing added a whole new set of potentially useful information. Depth of phase trees could be averaged to get an idea of a comment's sentence complexity on average (complexity as judged by the parser, which is not perfect). Additionally, while parts of speech tagging enabled us to slice out contexts with more scalpel like precision, parsing enabled gave us a flexible level of precision in establishing contexts. The added abstraction of verb phrases and noun phrases, along with arbitrarily complex contexts, allowed us to see the verb and noun phrases that contained certain words of interest. The SubRelation fuction also allowed us to pick out certain parts of speech in V/N phrases also with arbitrarily complex contexts.\n",
    "\n",
    "    Dependency parsing, at least as shown in the parsing section, did not have as many uses independent of information extraction. Information extraction sat on top of dependency parsing, and took up a fair amount of time and memory. (Information extraction could not be run on my computer even with only 20 comments without running out of memory.) However, the highly interpretable subject/verb/object labelling might be useful if run over more data."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

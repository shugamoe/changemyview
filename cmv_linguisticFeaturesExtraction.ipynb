{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All these packages need to be installed from pip\n",
    "import sklearn\n",
    "import sklearn.feature_extraction.text\n",
    "import requests #for http requests\n",
    "import nltk #the Natural Language Toolkit\n",
    "import pandas #gives us DataFrames\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "import wordcloud #Makes word clouds\n",
    "import numpy as np #For KL divergence\n",
    "import scipy #For KL divergence\n",
    "import seaborn as sns #makes our plots look nicer\n",
    "import sklearn.manifold #For a manifold plot\n",
    "from nltk.corpus import stopwords #For stopwords\n",
    "%matplotlib inline\n",
    "import os #For making directories\n",
    "import io #for making http requests look like files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words_nltk = nltk.corpus.stopwords.words('english')\n",
    "snowball = nltk.stem.snowball.SnowballStemmer('english')\n",
    "\n",
    "def normlizeTokens(tokenLst, stopwordLst = None, stemmer = None, lemmer = None, vocab = None):\n",
    "    #We can use a generator here as we just need to iterate over it\n",
    "\n",
    "    #Lowering the case and removing non-words\n",
    "    workingIter = (w.lower() for w in tokenLst if w.isalpha())\n",
    "\n",
    "    #Now we can use the semmer, if provided\n",
    "    if stemmer is not None:\n",
    "        workingIter = (stemmer.stem(w) for w in workingIter)\n",
    "\n",
    "    #And the lemmer\n",
    "    if lemmer is not None:\n",
    "        workingIter = (lemmer.lemmatize(w) for w in workingIter)\n",
    "\n",
    "    #And remove the stopwords\n",
    "    if stopwordLst is not None:\n",
    "        workingIter = (w for w in workingIter if w not in stopwordLst)\n",
    "        \n",
    "    #We will return a list with the stopwords removed\n",
    "    if vocab is not None:\n",
    "        vocab_str = '|'.join(vocab)\n",
    "        workingIter = (w for w in workingIter if re.match(vocab_str, w))\n",
    "    \n",
    "    return list(workingIter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pandas.read_pickle('data/cmv_data.pkl')\n",
    "    #edf = pandas.read_pickle('data/extrap.pkl')\n",
    "except:\n",
    "    df = pandas.read_pickle('cmv_data.pkl')\n",
    "    #edf = pandas.read_pickle('extrap.pkl')\n",
    "\n",
    "df = df.sample(frac = .1)\n",
    "\n",
    "# cols = np.intersect1d(df.columns, edf.columns)\n",
    "# edf = edf[cols]\n",
    "# df.append(edf)\n",
    "# df['exp'] = False\n",
    "# edf['exp'] = True\n",
    "# df = pd.concat([df, edf], axis=0)\n",
    "\n",
    "#take out rows where comment is '[removed]' or '[deleted]'\n",
    "df = df[(df['com_text']!='[deleted]')&(df['com_text']!='[removed]')]\n",
    "\n",
    "df['tokenized_com'] = df['com_text'].apply(lambda x: nltk.word_tokenize(x))\n",
    "df['normalized_com'] = df['tokenized_com'].apply(lambda x: normlizeTokens(x, stopwordLst = stop_words_nltk, stemmer = snowball))\n",
    "\n",
    "df['tokenized_sub'] = df['sub_text'].apply(lambda x: nltk.word_tokenize(x))\n",
    "df['normalized_sub'] = df['tokenized_sub'].apply(lambda x: normlizeTokens(x, stopwordLst = stop_words_nltk, stemmer = snowball))\n",
    "\n",
    "#splitting data\n",
    "# data_train, data_test = train_test_split(df, test_size=0.3, random_state=123)\n",
    "# data_train['is_train'] = True\n",
    "# data_test['is_train'] = False\n",
    "# edf = df[df['exp']==True]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub_text</th>\n",
       "      <th>com_text</th>\n",
       "      <th>com_delta_received</th>\n",
       "      <th>com_delta_from_op</th>\n",
       "      <th>com_upvotes</th>\n",
       "      <th>tokenized_com</th>\n",
       "      <th>normalized_com</th>\n",
       "      <th>tokenized_sub</th>\n",
       "      <th>normalized_sub</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>com_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cbwsi7d</th>\n",
       "      <td>People today are so easily offended by casual ...</td>\n",
       "      <td>The school can't really get away with this, bu...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>[The, school, ca, n't, really, get, away, with...</td>\n",
       "      <td>[school, ca, realli, get, away, far, parent, c...</td>\n",
       "      <td>[People, today, are, so, easily, offended, by,...</td>\n",
       "      <td>[peopl, today, easili, offend, casual, word, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cvaq71h</th>\n",
       "      <td>Two major arguments I have. \\n\\n1. In general,...</td>\n",
       "      <td>Sorry Shut_da_fuck_up_bot, your comment has be...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>[Sorry, Shut_da_fuck_up_bot, ,, your, comment,...</td>\n",
       "      <td>[sorri, comment, remov, comment, rule, rude, h...</td>\n",
       "      <td>[Two, major, arguments, I, have, ., 1, ., In, ...</td>\n",
       "      <td>[two, major, argument, general, rule, select, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cueu3av</th>\n",
       "      <td>First off, this is an exciting new subreddit f...</td>\n",
       "      <td>If a point of view is supported by group think...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>[If, a, point, of, view, is, supported, by, gr...</td>\n",
       "      <td>[point, view, support, group, think, doe, mean...</td>\n",
       "      <td>[First, off, ,, this, is, an, exciting, new, s...</td>\n",
       "      <td>[first, excit, new, subreddit, love, idea, how...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ceat8pa</th>\n",
       "      <td>You can sort comments on Reddit (submissions t...</td>\n",
       "      <td>Things can be controversial for many different...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>[Things, can, be, controversial, for, many, di...</td>\n",
       "      <td>[thing, controversi, mani, differ, reason, onl...</td>\n",
       "      <td>[You, can, sort, comments, on, Reddit, (, subm...</td>\n",
       "      <td>[sort, comment, reddit, submiss, believ, contr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cj3wpir</th>\n",
       "      <td>There's several mitigating factors to rich peo...</td>\n",
       "      <td>They don't *have* to give anything.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>[They, do, n't, *have*, to, give, anything, .]</td>\n",
       "      <td>[give, anyth]</td>\n",
       "      <td>[There, 's, several, mitigating, factors, to, ...</td>\n",
       "      <td>[sever, mitig, factor, rich, peopl, generos, w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  sub_text  \\\n",
       "com_id                                                       \n",
       "cbwsi7d  People today are so easily offended by casual ...   \n",
       "cvaq71h  Two major arguments I have. \\n\\n1. In general,...   \n",
       "cueu3av  First off, this is an exciting new subreddit f...   \n",
       "ceat8pa  You can sort comments on Reddit (submissions t...   \n",
       "cj3wpir  There's several mitigating factors to rich peo...   \n",
       "\n",
       "                                                  com_text com_delta_received  \\\n",
       "com_id                                                                          \n",
       "cbwsi7d  The school can't really get away with this, bu...              False   \n",
       "cvaq71h  Sorry Shut_da_fuck_up_bot, your comment has be...              False   \n",
       "cueu3av  If a point of view is supported by group think...              False   \n",
       "ceat8pa  Things can be controversial for many different...              False   \n",
       "cj3wpir                They don't *have* to give anything.              False   \n",
       "\n",
       "        com_delta_from_op  com_upvotes  \\\n",
       "com_id                                   \n",
       "cbwsi7d             False            1   \n",
       "cvaq71h             False            1   \n",
       "cueu3av             False            5   \n",
       "ceat8pa             False            2   \n",
       "cj3wpir             False            2   \n",
       "\n",
       "                                             tokenized_com  \\\n",
       "com_id                                                       \n",
       "cbwsi7d  [The, school, ca, n't, really, get, away, with...   \n",
       "cvaq71h  [Sorry, Shut_da_fuck_up_bot, ,, your, comment,...   \n",
       "cueu3av  [If, a, point, of, view, is, supported, by, gr...   \n",
       "ceat8pa  [Things, can, be, controversial, for, many, di...   \n",
       "cj3wpir     [They, do, n't, *have*, to, give, anything, .]   \n",
       "\n",
       "                                            normalized_com  \\\n",
       "com_id                                                       \n",
       "cbwsi7d  [school, ca, realli, get, away, far, parent, c...   \n",
       "cvaq71h  [sorri, comment, remov, comment, rule, rude, h...   \n",
       "cueu3av  [point, view, support, group, think, doe, mean...   \n",
       "ceat8pa  [thing, controversi, mani, differ, reason, onl...   \n",
       "cj3wpir                                      [give, anyth]   \n",
       "\n",
       "                                             tokenized_sub  \\\n",
       "com_id                                                       \n",
       "cbwsi7d  [People, today, are, so, easily, offended, by,...   \n",
       "cvaq71h  [Two, major, arguments, I, have, ., 1, ., In, ...   \n",
       "cueu3av  [First, off, ,, this, is, an, exciting, new, s...   \n",
       "ceat8pa  [You, can, sort, comments, on, Reddit, (, subm...   \n",
       "cj3wpir  [There, 's, several, mitigating, factors, to, ...   \n",
       "\n",
       "                                            normalized_sub  \n",
       "com_id                                                      \n",
       "cbwsi7d  [peopl, today, easili, offend, casual, word, c...  \n",
       "cvaq71h  [two, major, argument, general, rule, select, ...  \n",
       "cueu3av  [first, excit, new, subreddit, love, idea, how...  \n",
       "ceat8pa  [sort, comment, reddit, submiss, believ, contr...  \n",
       "cj3wpir  [sever, mitig, factor, rich, peopl, generos, w...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, top 40 bigrams of all comments\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('doe', 'mean'), 10.464401466807153),\n",
       " (('mani', 'peopl'), 8.953389675702558),\n",
       " (('feel', 'like'), 8.714150132028555),\n",
       " (('chang', 'view'), 8.114566852470945),\n",
       " (('lot', 'peopl'), 7.8446061015929756),\n",
       " (('make', 'sens'), 7.778129798216478),\n",
       " (('reason', 'whi'), 7.254474680915453),\n",
       " (('high', 'school'), 7.175853813161185),\n",
       " (('seem', 'like'), 7.165288180696215),\n",
       " (('even', 'though'), 7.1586704150516285),\n",
       " (('everyon', 'els'), 6.943722620109187),\n",
       " (('tl', 'dr'), 6.6315494649483595),\n",
       " (('first', 'place'), 6.630977507334409),\n",
       " (('let', 'say'), 6.572738958143521),\n",
       " (('year', 'old'), 6.440078483598249),\n",
       " (('pretti', 'much'), 6.384217373574809),\n",
       " (('sound', 'like'), 6.349588741984738),\n",
       " (('someon', 'els'), 6.307266325285223),\n",
       " (('black', 'peopl'), 6.242794093589202),\n",
       " (('year', 'ago'), 6.224239710933813),\n",
       " (('white', 'peopl'), 6.182869434747166),\n",
       " (('minimum', 'wage'), 6.160612242935322),\n",
       " (('someth', 'like'), 6.053584163791531),\n",
       " (('doe', 'make'), 5.803502968363975),\n",
       " (('men', 'women'), 5.787454368437821),\n",
       " (('make', 'sure'), 5.78674621183652),\n",
       " (('believ', 'god'), 5.7785050122117925),\n",
       " (('would', 'argu'), 5.763517971600898),\n",
       " (('unit', 'state'), 5.730734330028153),\n",
       " (('mental', 'health'), 5.727766405405176),\n",
       " (('thing', 'like'), 5.680799595433835),\n",
       " (('would', 'say'), 5.608695870438738),\n",
       " (('peopl', 'would'), 5.581978480299257),\n",
       " (('mental', 'ill'), 5.560432614294443),\n",
       " (('take', 'care'), 5.399124556258553),\n",
       " (('group', 'peopl'), 5.397262017275473),\n",
       " (('doe', 'matter'), 5.379056951535598),\n",
       " (('basic', 'incom'), 5.3703511422155055),\n",
       " (('would', 'like'), 5.294035786692793),\n",
       " (('gon', 'na'), 5.290536918551444)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Overview: just to get an idea of top bi/tri/quad/n-grams in all comments and the comments that receive delta\n",
    "print(\"First, top 40 bigrams of all comments\")\n",
    "allBigrams = nltk.collocations.BigramCollocationFinder.from_words(df['normalized_com'].sum())\n",
    "#print(\"There are {} bigrams in the finder\".format(cpBigrams.N))\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "allBigrams.score_ngrams(bigram_measures.student_t)[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of delta-receiving comments\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('minimum', 'wage'), 2.8234665542149067),\n",
       " (('child', 'support'), 2.8148982960245084),\n",
       " (('tl', 'dr'), 2.234285397798339),\n",
       " (('avoid', 'victim'), 2.212110106312292),\n",
       " (('rape', 'victim'), 2.1967086176917574),\n",
       " (('bin', 'laden'), 1.9987244897959184),\n",
       " (('oxford', 'comma'), 1.9984056122448979),\n",
       " (('appl', 'core'), 1.9980070153061225),\n",
       " (('punch', 'line'), 1.9977678571428572),\n",
       " (('financi', 'abort'), 1.990672831632653),\n",
       " (('larg', 'cake'), 1.9880420918367347),\n",
       " (('larg', 'group'), 1.9808673469387754),\n",
       " (('would', 'agre'), 1.9579081632653061),\n",
       " (('get', 'done'), 1.9508928571428572),\n",
       " (('mani', 'peopl'), 1.9426020408163265),\n",
       " (('would', 'like'), 1.7316645408163265),\n",
       " (('porch', 'light'), 1.7309461833293565),\n",
       " (('justic', 'movement'), 1.7306700272694762),\n",
       " (('wallet', 'stolen'), 1.7306700272694762),\n",
       " (('wast', 'materi'), 1.7305779752495163),\n",
       " (('feder', 'budget'), 1.7302097671696757),\n",
       " (('leav', 'porch'), 1.7301177151497156),\n",
       " (('steve', 'job'), 1.7301177151497156),\n",
       " (('social', 'justic'), 1.7295654030299552),\n",
       " (('held', 'account'), 1.7292892469700751),\n",
       " (('hold', 'account'), 1.7281846227305542),\n",
       " (('patern', 'test'), 1.7279084666706737),\n",
       " (('small', 'busi'), 1.7270799984910332),\n",
       " (('vote', 'method'), 1.7265276863712726),\n",
       " (('year', 'ago'), 1.7261594782914322),\n",
       " (('legal', 'process'), 1.7256071661716716),\n",
       " (('militari', 'spend'), 1.7256071661716716),\n",
       " (('everi', 'night'), 1.7254230621317515),\n",
       " (('keep', 'mind'), 1.7246866459720709),\n",
       " (('qualiti', 'reddit'), 1.7243184378922305),\n",
       " (('top', 'post'), 1.7237661257724701),\n",
       " (('around', 'week'), 1.7232138136527095),\n",
       " (('public', 'polici'), 1.7229376575928295),\n",
       " (('abort', 'window'), 1.721280721233548),\n",
       " (('bad', 'public'), 1.7209125131537075)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For delta-receiving comments\n",
    "print(\"Of delta-receiving comments\")\n",
    "deltaBigrams = nltk.collocations.BigramCollocationFinder.from_words(df[df['com_delta_received']]['normalized_com'].sum())\n",
    "deltaBigrams.score_ngrams(bigram_measures.student_t)[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('natur', 'way', 'thing'), 3.6043079879944746),\n",
       " (('mental', 'health', 'issu'), 3.4640394473164147),\n",
       " (('third', 'parti', 'candid'), 3.316609127317029),\n",
       " (('cultur', 'institut', 'marriag'), 2.9999873106581014),\n",
       " (('pay', 'child', 'support'), 2.9997932423411844),\n",
       " (('doe', 'make', 'sens'), 2.997893625517289),\n",
       " (('world', 'war', 'ii'), 2.828420460658309),\n",
       " (('destroy', 'cultur', 'institut'), 2.828420395206411),\n",
       " (('knowledg', 'good', 'evil'), 2.8283890818812676),\n",
       " (('vote', 'third', 'parti'), 2.828362610343406),\n",
       " (('mental', 'health', 'problem'), 2.828333498391499),\n",
       " (('black', 'live', 'matter'), 2.8281821763851536),\n",
       " (('larg', 'group', 'peopl'), 2.8273620739195837),\n",
       " (('make', 'feel', 'like'), 2.820159672331049),\n",
       " (('modern', 'western', 'nation'), 2.6457476658900743),\n",
       " (('lesser', 'two', 'evil'), 2.6457460861592708),\n",
       " (('look', 'modern', 'western'), 2.6457425131433814),\n",
       " (('western', 'nation', 'number'), 2.645741318258589),\n",
       " (('number', 'sexual', 'partner'), 2.6457345741426046),\n",
       " (('sexual', 'partner', 'around'), 2.645726679368083)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trigrams\n",
    "#For all\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "allTrigrams = nltk.collocations.TrigramCollocationFinder.from_words(df['normalized_com'].sum())\n",
    "allTrigrams.score_ngrams(trigram_measures.student_t)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('leav', 'porch', 'light'), 1.7320495747293245),\n",
       " (('social', 'justic', 'movement'), 1.7320488262195957),\n",
       " (('bad', 'public', 'polici'), 1.732034824684672),\n",
       " (('child', 'support', 'child'), 1.731997971588033),\n",
       " (('pay', 'child', 'support'), 1.731983881993141),\n",
       " (('mother', 'ayn', 'rand'), 1.414213130969118),\n",
       " (('iowa', 'code', 'ann'), 1.4142129871677922),\n",
       " (('class', 'action', 'lawsuit'), 1.4142124838631522),\n",
       " (('design', 'cover', 'store'), 1.4142122681611635),\n",
       " (('critic', 'race', 'theori'), 1.414211944608181),\n",
       " (('legal', 'process', 'immigr'), 1.4142110458498953),\n",
       " (('keep', 'door', 'open'), 1.4142103268432666),\n",
       " (('critic', 'muslim', 'nation'), 1.4142097875882953),\n",
       " (('detract', 'qualiti', 'reddit'), 1.4142090326313355),\n",
       " (('know', 'provid', 'alcohol'), 1.41420612065449),\n",
       " (('best', 'case', 'scenario'), 1.4142057971015074),\n",
       " (('ensur', 'resourc', 'go'), 1.4142043590882503),\n",
       " (('safe', 'option', 'critic'), 1.414204125411096),\n",
       " (('victim', 'punch', 'line'), 1.4142014830617362),\n",
       " (('avoid', 'servic', 'happen'), 1.4141999731478165)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Delta-receiving\n",
    "#trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "deltaTrigrams = nltk.collocations.TrigramCollocationFinder.from_words(df[df['com_delta_received']]['normalized_com'].sum())\n",
    "deltaTrigrams.score_ngrams(trigram_measures.student_t)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df['bigrams'] = df['normalized_com'].apply(lambda x: nltk.collocations.BigramCollocationFinder.from_words(x))\n",
    "# df['bigrams'] = df['bigrams'].apply(lambda x: x.score_ngrams(bigram_measures.student_t)[:10])\n",
    "# df['bigrams'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF feature matrix\n",
    "and PCA to reduce dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2878, 1000)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialize tf-idf feature matrix\n",
    "ngramTFVectorizer = sklearn.feature_extraction.text.TfidfVectorizer(max_df=0.5, max_features=1000, min_df=3,ngram_range=(1, 10), stop_words='english', norm='l2')\n",
    "#train\n",
    "ngramTFVects = ngramTFVectorizer.fit_transform(df['com_text'])\n",
    "ngramTFVects.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use PCA to reduce the feature matrix\n",
    "PCA = sklearn.decomposition.PCA\n",
    "pca = PCA().fit(ngramTFVects.toarray())\n",
    "reduced_data = pca.transform(ngramTFVects.toarray())\n",
    "\n",
    "# PCA = sklearn.decomposition.PCA\n",
    "# pca = PCA().fit(subTFVects.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pca.explained_variance_ratio_.shape\n",
    "#ngramTFVects.toarray().shape\n",
    "pca.explained_variance_ratio_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-22a4696b332c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# print(eigen_vals.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# print(pca.explained_variance_ratio_.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meigen_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ro-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Scree Plot'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Principal Component'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1817\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1818\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1819\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1820\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1383\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_grab_next_args\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mseg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mseg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x and y must have same first dimension\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x and y can be no greater than 2-D\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeUAAAG2CAYAAABBKp47AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8068047ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#use scree plot to determine the number of dimensions\n",
    "n = ngramTFVects.shape[0]\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "ax1 = fig.add_subplot(121)\n",
    "eigen_vals = np.arange(n) + 1\n",
    "\n",
    "# print(eigen_vals.shape)\n",
    "# print(pca.explained_variance_ratio_.shape)\n",
    "ax1.plot(eigen_vals, pca.explained_variance_ratio_, 'ro-', linewidth=2)\n",
    "ax1.set_title('Scree Plot')\n",
    "ax1.set_xlabel('Principal Component')\n",
    "ax1.set_ylabel('Proportion of Explained Variance')\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "eigen_vals = np.arange(20) + 1\n",
    "ax2.plot(eigen_vals, pca.explained_variance_ratio_[:20], 'ro-', linewidth=2)\n",
    "ax2.set_title('Scree Plot (First 20 Principal Components)')\n",
    "ax2.set_xlabel('Principal Component')\n",
    "ax2.set_ylabel('Proportion of Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = reduced_data[:, :15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KL and JS divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KL\n",
    "Overview part: Let's compare KL divergence between delta-receiving comments and the others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#set(df['normalized_sub'][0])\n",
    "#df['normalized_sub'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeProbsArray(dfColumn, overlapDict):\n",
    "    words = dfColumn.sum()\n",
    "    countList = [0] * len(overlapDict)\n",
    "    for word in words:\n",
    "        try:\n",
    "            countList[overlapDict[word]] += 1\n",
    "        except KeyError:\n",
    "            #The word is not common so we skip it\n",
    "            pass\n",
    "    countArray = np.array(countList)\n",
    "    return countArray / countArray.sum()\n",
    "\n",
    "def comparison(df_a, df_b):\n",
    "    words_a = set(df_a['normalized_com'].sum())\n",
    "    words_b = set(df_a['normalized_com'].sum())\n",
    "    #Change & to | if you want to keep all words\n",
    "    overlapWords = words_a & words_b\n",
    "    overlapWordsDict = {word: index for index, word in enumerate(overlapWords)}\n",
    "    #print(overlapWordsDict['student'])\n",
    "    \n",
    "    aProbArray = makeProbsArray(df_a['normalized_com'], overlapWordsDict)\n",
    "    bProbArray = makeProbsArray(df_b['normalized_com'], overlapWordsDict)\n",
    "    return (aProbArray, bProbArray,overlapWordsDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>elementwise divergence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>quadrillion</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>gmt</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1428</th>\n",
       "      <td>vestigi</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>notifi</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444</th>\n",
       "      <td>cesspool</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>squeak</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>tortilla</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>cecil</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>douchy</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>extrapol</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             word  elementwise divergence\n",
       "954   quadrillion                     inf\n",
       "416           gmt                     inf\n",
       "1428      vestigi                     inf\n",
       "439        notifi                     inf\n",
       "1444     cesspool                     inf\n",
       "1452       squeak                     inf\n",
       "425      tortilla                     inf\n",
       "418         cecil                     inf\n",
       "1459       douchy                     inf\n",
       "456      extrapol                     inf"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Elementwise divergence and see which words best distinguish the two corpora\n",
    "tupl = comparison(df[df['com_delta_received']],df[df['com_delta_received']==False])\n",
    "d_ndDivergence_ew = scipy.special.kl_div(*tupl[:2])\n",
    "dic = tupl[2]\n",
    "kl_df = pandas.DataFrame(list(dic.keys()), columns = ['word'], index = list(dic.values())).sort_index()\n",
    "kl_df['elementwise divergence'] = d_ndDivergence_ew\n",
    "kl_df.sort_values(by='elementwise divergence', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KL feature column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['tuple'] = list(zip(df['sub_text'], df['com_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _kldiv(A, B):\n",
    "    return np.sum([v for v in A * np.log2(A/B) if not np.isnan(v)])\n",
    "\n",
    "\n",
    "\n",
    "def make_prob_array(norm_toks, overlap_dict):\n",
    "        count_list = [0] * len(overlap_dict)\n",
    "\n",
    "        for tok in norm_toks:\n",
    "            try:\n",
    "                count_list[overlap_dict[tok]] += 1\n",
    "            except KeyError:\n",
    "                pass\n",
    "        count_array = np.array(count_list)\n",
    "        return(count_array / count_array.sum())\n",
    "\n",
    "\n",
    "def calc_kl_divergence(string1, string2):\n",
    "    '''\n",
    "    Calculates the kl Divergence between two sets of strings\n",
    "    '''\n",
    "    norm_toks1 = normlizeTokens(nltk.word_tokenize(string1), stopwordLst = stop_words_nltk, stemmer =  snowball)\n",
    "    norm_toks2 = normlizeTokens(nltk.word_tokenize(string2), stopwordLst = stop_words_nltk, stemmer =  snowball)\n",
    "    \n",
    "    words1 = set(norm_toks1)\n",
    "    words2 = set(norm_toks2)\n",
    "\n",
    "    overlap_words = words1 & words2\n",
    "    overlap_words_dict = {word: index for index, word in enumerate(overlap_words)}\n",
    "\n",
    "    prob1 = make_prob_array(norm_toks1, overlap_words_dict)\n",
    "    prob2 = make_prob_array(norm_toks2, overlap_words_dict)\n",
    "\n",
    "    kl_div = _kldiv(prob1, prob2)\n",
    "    return(kl_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['KL'] = df['tuple'].apply(lambda x: calc_kl_divergence(x[0], x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JS divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From http://stackoverflow.com/questions/15880133/jensen-shannon-divergence\n",
    "def jsdiv(P, Q):\n",
    "    \"\"\"Compute the Jensen-Shannon divergence between two probability distributions.\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    P, Q : array-like\n",
    "        Probability distributions of equal length that sum to 1\n",
    "    \"\"\"\n",
    "\n",
    "    def _kldiv(A, B):\n",
    "        return np.sum([v for v in A * np.log2(A/B) if not np.isnan(v)])\n",
    "\n",
    "    P = np.array(P)\n",
    "    Q = np.array(Q)\n",
    "\n",
    "    M = 0.5 * (P + Q)\n",
    "\n",
    "    return 0.5 * (_kldiv(P, M) +_kldiv(Q, M))\n",
    "\n",
    "\n",
    "def make_prob_array(norm_toks, overlap_dict):\n",
    "        count_list = [0] * len(overlap_dict)\n",
    "\n",
    "        for tok in norm_toks:\n",
    "            try:\n",
    "                count_list[overlap_dict[tok]] += 1\n",
    "            except KeyError:\n",
    "                pass\n",
    "        count_array = np.array(count_list)\n",
    "        return(count_array / count_array.sum())\n",
    "\n",
    "\n",
    "def calc_JS_divergence(string1, string2):\n",
    "    '''\n",
    "    Calculates the Jensen Shannon Divergence between two sets of strings\n",
    "    '''\n",
    "    norm_toks1 = normlizeTokens(nltk.word_tokenize(string1), stopwordLst = stop_words_nltk, stemmer =  snowball)\n",
    "    norm_toks2 = normlizeTokens(nltk.word_tokenize(string2), stopwordLst = stop_words_nltk, stemmer =  snowball)\n",
    "    \n",
    "    words1 = set(norm_toks1)\n",
    "    words2 = set(norm_toks2)\n",
    "\n",
    "    overlap_words = words1 & words2\n",
    "    overlap_words_dict = {word: index for index, word in enumerate(overlap_words)}\n",
    "\n",
    "    prob1 = make_prob_array(norm_toks1, overlap_words_dict)\n",
    "    prob2 = make_prob_array(norm_toks2, overlap_words_dict)\n",
    "\n",
    "    js_div = jsdiv(prob1, prob2)\n",
    "    return(js_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "df['JS'] = df['tuple'].apply(lambda x: calc_JS_divergence(x[0], x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[df['com_delta_received']]['JS'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[df['com_delta_received']==False]['JS'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[df['com_delta_received']]['KL'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[df['com_delta_received']==False]['KL'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delta-receiving comments have significantly higher JS divergence from the original posts than the other comments; they also have much higher KL divergence. \n",
    "\n",
    "Maybe this shows that these comments are able to raise more angles/perspectives?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
